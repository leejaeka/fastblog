{
  
    
        "post0": {
            "title": "Renekton Croc plush detector",
            "content": "LET&#39;S BUILD A CROCODILE, PLUSH, RENEKTON DETECTOR . import os key = os.environ.get(&#39;AZURE_SEARCH_KEY&#39;, &#39;1a25802a09ab45e4a082267f88ee5bd1&#39;) . reptile_types = &#39;crocodile&#39;,&#39;alligator plush&#39;, &#39;renekton&#39; path = Path(&#39;reptiles&#39;) . if not path.exists(): path.mkdir() for o in reptile_types: dest = (path/o) dest.mkdir(exist_ok=True) results = search_images_bing(key, f&#39;{o}&#39;) download_images(dest, urls=results.attrgot(&#39;content_url&#39;)) . fns = get_image_files(path) fns . (#416) [Path(&#39;reptiles/alligator plush/00000000.jpg&#39;),Path(&#39;reptiles/alligator plush/00000001.jpg&#39;),Path(&#39;reptiles/alligator plush/00000002.jpg&#39;),Path(&#39;reptiles/alligator plush/00000003.jpg&#39;),Path(&#39;reptiles/alligator plush/00000004.jpeg&#39;),Path(&#39;reptiles/alligator plush/00000005.jpg&#39;),Path(&#39;reptiles/alligator plush/00000006.jpg&#39;),Path(&#39;reptiles/alligator plush/00000007.jpg&#39;),Path(&#39;reptiles/alligator plush/00000008.jpg&#39;),Path(&#39;reptiles/alligator plush/00000009.jpg&#39;)...] . failed = verify_images(fns) failed.map(Path.unlink) . (#0) [] . class DataLoaders(GetAttr): num_workers=0 def __init__(self, *loaders): self.loaders = loaders def __getitem__(self, i): return self.loaders[i] train,valid = add_props(lambda i, self: self[i]) . reptiles = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(128)) . dls = reptiles.dataloaders(path) dls.valid.show_batch(max_n=4, nrows=1) . reptiles = reptiles.new(item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) dls = reptiles.dataloaders(path, num_workers=0) # &lt;- num_workers=0 to prevent window error . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . epoch train_loss valid_loss error_rate time . 0 | 1.455003 | 0.249432 | 0.084337 | 00:18 | . epoch train_loss valid_loss error_rate time . 0 | 0.104521 | 0.049535 | 0.024096 | 00:18 | . 1 | 0.068319 | 0.012980 | 0.012048 | 00:18 | . 2 | 0.052283 | 0.011862 | 0.000000 | 00:19 | . 3 | 0.041685 | 0.010840 | 0.000000 | 00:19 | . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . Result is very good . # cleaner # for idx in cleaner.delete(): cleaner.fns[idx].unlink() # delete . Let&#39;s test . my_renek = PILImage.create(&quot;renek_plush.png&quot;) display(my_renek.to_thumb(256,256)) . pred, pred_idx, probs =learn.predict(my_renek) f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; . &#39;Prediction: alligator plush; Probability: 0.9391&#39; . Very good. It is very accurate since my drawing of a plush is very realistic. . renek = PILImage.create(&quot;renek_test.png&quot;) display(renek.to_thumb(256,256)) pred, pred_idx, probs =learn.predict(renek) f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; . &#39;Prediction: renekton; Probability: 0.9834&#39; . Easily recognizes my drawing of Renekton as well. I guess I&#39;m an artist . renek_withoutbg = PILImage.create(&quot;renek_test1.png&quot;) display(renek_withoutbg.to_thumb(256,256)) pred, pred_idx, probs =learn.predict(renek_withoutbg) f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; . &#39;Prediction: renekton; Probability: 0.9674&#39; . Expected the model to predict plush becasue I removed the background but it&#39;s too smart. (In dataset a lot of plush had empty white background contrast to lots of Renekton images having dark backgrounds) . beard = PILImage.create(&quot;beard.jpg&quot;) display(beard.to_thumb(200,200)) pred, pred_idx, probs =learn.predict(beard) f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; . &#39;Prediction: alligator plush; Probability: 0.8644&#39; . Indeed I am an alligator plush with my fake beard! . learn.export() . RUN CODE BELOW TO MAKE YOUR OWN TEST (CLICK UPLOAD) . from fastai.vision.widgets import * btn_upload = widgets.FileUpload() out_pl = widgets.Output() lbl_pred = widgets.Label() . path = Path(&#39;&#39;) learn_inf = load_learner(path/&#39;export.pkl&#39;, cpu=True) . def on_data_change(change): lbl_pred.value = &#39;&#39; img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) pred,pred_idx,probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; . btn_upload.observe(on_data_change, names=[&#39;data&#39;]) display(VBox([widgets.Label(&#39;Feed me a reptile photo!&#39;), btn_upload, out_pl, lbl_pred])) .",
            "url": "https://leejaeka.github.io/fastblog/fastpages/jupyter/2020/10/27/croc.html",
            "relUrl": "/fastpages/jupyter/2020/10/27/croc.html",
            "date": " • Oct 27, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "City Detector",
            "content": "Let&#39;s make app that recognizes Coquitlam! . import os key = os.environ.get(&#39;AZURE_SEARCH_KEY&#39;, &#39;39f25aae8d744a528b964a94a4af8b58&#39;) . Let&#39;s get the images of each city . city_types = &#39;seoul city&#39;,&#39;coquitlam&#39;,&#39;paris city&#39;, &#39;new york city&#39; path = Path(&#39;cities&#39;) . if not path.exists(): path.mkdir() for o in city_types: dest = (path/o) dest.mkdir(exist_ok=True) results = search_images_bing(key, f&#39;{o}&#39;) download_images(dest, urls=results.attrgot(&#39;content_url&#39;)) . fns = get_image_files(path) fns . (#596) [Path(&#39;cities/coquitlam/00000000.jpg&#39;),Path(&#39;cities/coquitlam/00000001.jpg&#39;),Path(&#39;cities/coquitlam/00000002.png&#39;),Path(&#39;cities/coquitlam/00000003.jpg&#39;),Path(&#39;cities/coquitlam/00000004.jpg&#39;),Path(&#39;cities/coquitlam/00000005.jpg&#39;),Path(&#39;cities/coquitlam/00000006.jpg&#39;),Path(&#39;cities/coquitlam/00000007.jpg&#39;),Path(&#39;cities/coquitlam/00000008.jpg&#39;),Path(&#39;cities/coquitlam/00000009.jpg&#39;)...] . failed = verify_images(fns) failed . (#16) [Path(&#39;cities/coquitlam/00000067.jpg&#39;),Path(&#39;cities/coquitlam/00000077.JPG&#39;),Path(&#39;cities/coquitlam/00000079.jpg&#39;),Path(&#39;cities/coquitlam/00000135.jpg&#39;),Path(&#39;cities/new york city/00000010.jpg&#39;),Path(&#39;cities/new york city/00000014.jpg&#39;),Path(&#39;cities/new york city/00000020.jpg&#39;),Path(&#39;cities/new york city/00000026.jpg&#39;),Path(&#39;cities/new york city/00000029.jpg&#39;),Path(&#39;cities/new york city/00000037.jpg&#39;)...] . failed.map(Path.unlink) . (#16) [None,None,None,None,None,None,None,None,None,None...] . Let&#39;s load the data . class DataLoaders(GetAttr): def __init__(self, *loaders): self.loaders = loaders def __getitem__(self, i): return self.loaders[i] train,valid = add_props(lambda i, self: self[i]) . cities = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(128)) . dls = cities.dataloaders(path) dls.valid.show_batch(max_n=8, nrows = 2) . Let&#39;s scale and augment the datas . cities = cities.new(item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) dls = cities.dataloaders(path, num_workers=0) # &lt;- num_workers=0 to prevent window error . Let&#39;s build and run a CNN model . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . epoch train_loss valid_loss error_rate time . 0 | 2.187395 | 1.315027 | 0.482759 | 00:42 | . epoch train_loss valid_loss error_rate time . 0 | 1.307873 | 0.871226 | 0.336207 | 00:41 | . 1 | 1.064780 | 0.831430 | 0.241379 | 00:41 | . 2 | 0.876646 | 0.767134 | 0.215517 | 00:41 | . 3 | 0.784991 | 0.738216 | 0.224138 | 00:49 | . Let&#39;s look at the confusion matrix . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . We got an accuracy of 26/90 = 71% (rounded) . interp.plot_top_losses(2,nrows=2) . Let&#39;s try to clean up the dataset . cleaner = ImageClassifierCleaner(learn) cleaner . for idx in cleaner.delete(): cleaner.fns[idx].unlink() # delete . dls = cities.dataloaders(path, num_workers=0) learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(5) . epoch train_loss valid_loss error_rate time . 0 | 1.933403 | 1.322945 | 0.460870 | 00:38 | . epoch train_loss valid_loss error_rate time . 0 | 1.258363 | 0.800413 | 0.347826 | 00:37 | . 1 | 1.014135 | 0.660854 | 0.243478 | 00:38 | . 2 | 0.851025 | 0.609896 | 0.243478 | 00:38 | . 3 | 0.725140 | 0.591347 | 0.217391 | 00:37 | . 4 | 0.623130 | 0.582418 | 0.226087 | 00:37 | . Valid_loss doesn&#39;t decrease so we stop . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . interp.plot_top_losses(6,nrows=2) . It looks like it has a hard time highlighting seoul city&#39;s characteristics as most error comes from seoul images. Suspected factors include seoul having new york like buildings, mountains like coquitlam and brick structures like paris city. . Ignore Below (Deployment IPR) . learn.export() . path = Path() path.ls(file_exts=&#39;.pkl&#39;) . (#1) [Path(&#39;export.pkl&#39;)] . learn_inf = load_learner(path/&#39;export.pkl&#39;) . learn_inf.dls.vocab . [&#39;coquitlam&#39;, &#39;new york city&#39;, &#39;paris city&#39;, &#39;seoul city&#39;] . btn_upload = widgets.FileUpload() btn_upload . img = PILImage.create(btn_upload.data[-1]) . out_pl = widgets.Output() out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) out_pl . pred,pred_idx,probs = learn_inf.predict(img) #hide_output lbl_pred = widgets.Label() lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; lbl_pred . Classify Button &amp; Event Handler . btn_run = widgets.Button(description=&#39;Classify&#39;) btn_run . def on_click_classify(change): img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) pred,pred_idx,probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; btn_run.on_click(on_click_classify) .",
            "url": "https://leejaeka.github.io/fastblog/fastpages/jupyter/2020/10/27/city.html",
            "relUrl": "/fastpages/jupyter/2020/10/27/city.html",
            "date": " • Oct 27, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://leejaeka.github.io/fastblog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://leejaeka.github.io/fastblog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Yes it is I! .",
          "url": "https://leejaeka.github.io/fastblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://leejaeka.github.io/fastblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}