{
  
    
        "post0": {
            "title": "LoL Prediction S10",
            "content": "Introduction . Let&#39;s predict who won the match given team composition and how long game played out . Get dataset . The dataset is a collection of League of Legends High Elo(Challenger, GM, Master, High Diamonds) Ranked games in Season 10, Korea(WWW), North America(NA), Eastern Europe(EUNE), and Western Europe(EUW) servers. These datas were collected from op.gg by web scrapping with python spyder. The latest game was played on Oct.16th on the dataset. In total there are 4028 unique games. Note that I&#39;ve used one-hot encoding hence [99,54,101,73,57,96,52,102,68,52] this list represents number of all unique champions used in each lanes [BlueTop, BlueJG, BlueMid, BlueAdc, BlueSup, RedTop, RedJg, RedMid, RedAdc, RedSup] respectivley. Note that there are in total 151 unique champions with &#39;Samira&#39; as the latest addition. . import pandas as pd df = pd.read_csv(&quot;games.csv&quot;) . Some Setups . import sys assert sys.version_info &gt;= (3, 5) # Scikit-Learn ≥0.20 is required import sklearn assert sklearn.__version__ &gt;= &quot;0.20&quot; try: # %tensorflow_version only exists in Colab. %tensorflow_version 2.x except Exception: pass # TensorFlow ≥2.0 is required import tensorflow as tf from tensorflow import keras assert tf.__version__ &gt;= &quot;2.0&quot; %load_ext tensorboard # Common imports import numpy as np import os # to make this notebook&#39;s output stable across runs np.random.seed(42) # To plot pretty figures %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt mpl.rc(&#39;axes&#39;, labelsize=14) mpl.rc(&#39;xtick&#39;, labelsize=12) mpl.rc(&#39;ytick&#39;, labelsize=12) # Where to save the figures PROJECT_ROOT_DIR = &quot;.&quot; CHAPTER_ID = &quot;deep&quot; IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, &quot;images&quot;, CHAPTER_ID) os.makedirs(IMAGES_PATH, exist_ok=True) def save_fig(fig_id, tight_layout=True, fig_extension=&quot;png&quot;, resolution=300): path = os.path.join(IMAGES_PATH, fig_id + &quot;.&quot; + fig_extension) print(&quot;Saving figure&quot;, fig_id) if tight_layout: plt.tight_layout() plt.savefig(path, format=fig_extension, dpi=resolution) . The tensorboard extension is already loaded. To reload it, use: %reload_ext tensorboard . df.head(5) # First look at our dataset. Game_length includes some annoying string values instead of time value . game_length mmr result server team_1 team_2 timestamp . 0 25m 38s | NaN | Victory | na | Riven,Nidalee,Galio,Jhin,Pantheon | Camille,Olaf,Cassiopeia,Ezreal,Alistar | 2020-10-13 09:31:42 | . 1 25m 38s | NaN | Defeat | na | Teemo,Nidalee,Lucian,Caitlyn,Senna | Irelia,Hecarim,Cassiopeia,Jinx,Lulu | 2020-10-13 06:00:17 | . 2 25m 38s | NaN | Defeat | na | Malphite,Olaf,Taliyah,Ezreal,Alistar | Sylas,Lillia,Lucian,Senna,Pantheon | 2020-10-13 05:06:45 | . 3 25m 38s | NaN | Defeat | na | Neeko,Shen,Orianna,Kai&#39;Sa,Nautilus | Riven,Hecarim,Cassiopeia,Samira,Morgana | 2020-10-13 04:28:00 | . 4 25m 38s | NaN | Defeat | na | Fiora,Nunu &amp; Willump,Irelia,Jhin,Karma | Renekton,Elise,Kled,Jinx,Morgana | 2020-10-13 04:00:51 | . temp_df = df[[&#39;game_length&#39;, &#39;result&#39;, &#39;team_1&#39;, &#39;team_2&#39;]] # Select only interests blue = temp_df[&#39;team_1&#39;] red = temp_df[&#39;team_2&#39;] n = len(df) blue_champs = [] red_champs = [] for i in range(0,n): blue_champs += [blue[i].split(&#39;,&#39;)] red_champs += [red[i].split(&#39;,&#39;)] top = [] jg = [] mid = [] adc = [] sup = [] for i in range(0, n): top += [blue_champs[i][0]] jg += [blue_champs[i][1]] mid += [blue_champs[i][2]] adc += [blue_champs[i][3]] sup += [blue_champs[i][4]] top_2 = [] jg_2 = [] mid_2 = [] adc_2 = [] sup_2 = [] for i in range(0, n): top_2 += [red_champs[i][0]] jg_2 += [red_champs[i][1]] mid_2 += [red_champs[i][2]] adc_2 += [red_champs[i][3]] sup_2 += [red_champs[i][4]] . data = temp_df.drop(columns=[&#39;team_1&#39;,&#39;team_2&#39;]) # blue team data[&#39;top1&#39;] = top data[&#39;jg1&#39;] = jg data[&#39;mid1&#39;] = mid data[&#39;adc1&#39;] = adc data[&#39;sup1&#39;] = sup # red team data[&#39;top2&#39;] = top_2 data[&#39;jg2&#39;] = jg_2 data[&#39;mid2&#39;] = mid_2 data[&#39;adc2&#39;] = adc_2 data[&#39;sup2&#39;] = sup_2 . data.head(10) . game_length result top1 jg1 mid1 adc1 sup1 top2 jg2 mid2 adc2 sup2 . 0 25m 38s | Victory | Riven | Nidalee | Galio | Jhin | Pantheon | Camille | Olaf | Cassiopeia | Ezreal | Alistar | . 1 25m 38s | Defeat | Teemo | Nidalee | Lucian | Caitlyn | Senna | Irelia | Hecarim | Cassiopeia | Jinx | Lulu | . 2 25m 38s | Defeat | Malphite | Olaf | Taliyah | Ezreal | Alistar | Sylas | Lillia | Lucian | Senna | Pantheon | . 3 25m 38s | Defeat | Neeko | Shen | Orianna | Kai&#39;Sa | Nautilus | Riven | Hecarim | Cassiopeia | Samira | Morgana | . 4 25m 38s | Defeat | Fiora | Nunu &amp; Willump | Irelia | Jhin | Karma | Renekton | Elise | Kled | Jinx | Morgana | . 5 25m 38s | Defeat | Irelia | Karthus | Sylas | Samira | Nautilus | Riven | Kayn | Akali | Miss Fortune | Galio | . 6 25m 38s | Defeat | Galio | Kindred | Syndra | Ezreal | Blitzcrank | Camille | Fiddlesticks | Twisted Fate | Jhin | Morgana | . 7 25m 38s | Defeat | Poppy | Ekko | Sylas | Samira | Blitzcrank | Lucian | Lillia | Lulu | Caitlyn | Alistar | . 8 25m 38s | Defeat | Shen | Lillia | Samira | Lucian | Soraka | Taric | Master Yi | Riven | Ezreal | Lulu | . 9 25m 38s | Defeat | Ornn | Graves | Sylas | Lucian | Alistar | Irelia | Hecarim | Akali | Senna | Leona | . from sklearn.preprocessing import OneHotEncoder #y = pd.get_dummies(data.top1, prefix=&#39;top1&#39;) enc = OneHotEncoder() only_champs = data.drop(columns=[&#39;game_length&#39;, &#39;result&#39;]) only_champs.head(5) only_champs_onehot = enc.fit_transform(only_champs) . enc.get_params() . {&#39;categories&#39;: &#39;auto&#39;, &#39;drop&#39;: None, &#39;dtype&#39;: numpy.float64, &#39;handle_unknown&#39;: &#39;error&#39;, &#39;sparse&#39;: True} . import re date_str = data.game_length m = 2717 #longest games are 45m 17s for i in range(len(date_str)): if type(date_str[i]) == str: p = re.compile(&#39; d*&#39;) min = float(p.findall(date_str[i][:2])[0]) temp = p.findall(date_str[i][-3:]) for j in temp: if j != &#39;&#39;: sec = float(j) break date_str[i] = (60*min+sec)/m else: date_str[i] = date_str[i]/m # print(date_str[i]) # print(len(date_str)) . #except_champs = data.drop(columns=[&#39;result&#39;,&#39;top1&#39;,&#39;jg1&#39;,&#39;mid1&#39;,&#39;adc1&#39;,&#39;sup1&#39;,&#39;top2&#39;,&#39;jg2&#39;,&#39;mid2&#39;,&#39;adc2&#39;,&#39;sup2&#39;]) sparse_to_df = pd.DataFrame.sparse.from_spmatrix(only_champs_onehot) print(sparse_to_df.shape) print(date_str.shape) X = date_str.to_frame().join(sparse_to_df).dropna() X = np.asarray(X).astype(&#39;float32&#39;) . (4028, 754) (4028,) . y = data[&#39;result&#39;] for i in range(len(y)): if y[i] == &quot;Victory&quot;: y[i] = 1 else: y[i] = 0 . y = np.asarray(y).astype(&#39;float32&#39;) . Datas are one hot encoded and cleaned up. Let&#39;s train test split . from sklearn.model_selection import train_test_split import math X_train_full, X_test, y_train_full, y_test = train_test_split(X,y,test_size=0.2, random_state=42) #len(X_train) = 3222 l = math.floor(3222*0.8) X_valid, X_train = X_train_full[:l], X_train_full[l:] y_valid, y_train = y_train_full[:l], y_train_full[l:] print(y_valid.shape) print(X_valid.shape) . (2577,) (2577, 755) . Let&#39;s try Neural Network with dropouts . model = keras.models.Sequential([ keras.layers.Flatten(input_shape=(755,)), keras.layers.Dense(30, activation=&quot;relu&quot;, name=&quot;layer_1&quot;), keras.layers.Dropout(rate=0.2), keras.layers.Dense(16, activation=&quot;relu&quot;, name=&quot;layer_2&quot;), keras.layers.Dropout(rate=0.2), keras.layers.Dense(16, activation=&quot;relu&quot;, name=&quot;layer_3&quot;), keras.layers.Dropout(rate=0.2), keras.layers.Dense(1, activation=&quot;sigmoid&quot;, name=&quot;layer_4&quot;) ]) . model.compile(loss=&quot;binary_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics=[&quot;accuracy&quot;]) model.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten (Flatten) (None, 755) 0 _________________________________________________________________ layer_1 (Dense) (None, 30) 22680 _________________________________________________________________ dropout (Dropout) (None, 30) 0 _________________________________________________________________ layer_2 (Dense) (None, 16) 496 _________________________________________________________________ dropout_1 (Dropout) (None, 16) 0 _________________________________________________________________ layer_3 (Dense) (None, 16) 272 _________________________________________________________________ dropout_2 (Dropout) (None, 16) 0 _________________________________________________________________ layer_4 (Dense) (None, 1) 17 ================================================================= Total params: 23,465 Trainable params: 23,465 Non-trainable params: 0 _________________________________________________________________ . model.fit(X_train, y_train, epochs=50, batch_size=1) . test_loss, test_acc = model.evaluate(X_test, y_test) print(&#39;accuracy&#39;, test_acc) . 26/26 [==============================] - 0s 806us/step - loss: 3.8032 - accuracy: 0.6613 accuracy 0.6612903475761414 . We got about 0.661 accuracy with just raw neural network with dropouts. . Let&#39;s try random forests . from sklearn.ensemble import RandomForestClassifier rnd_clf = RandomForestClassifier(n_estimators=2000, max_leaf_nodes=32, n_jobs=-1) rnd_clf.fit(X_train, y_train) . RandomForestClassifier(max_leaf_nodes=32, n_estimators=2000, n_jobs=-1) . y_val_pred = rnd_clf.predict(X_valid) . val_acc = np.sum(y_val_pred == y_valid)/len(y_valid) print(&quot;validation accuracy: &quot;+str(val_acc)) . validation accuracy: 0.7710516103996896 . y_test_pred = rnd_clf.predict(X_test) test_acc = np.sum(y_test_pred == y_test)/len(y_test) print(&quot;test accuracy: &quot;+str(test_acc)) . test accuracy: 0.7704714640198511 . Immediate improvement by almost 10% with random forest classifier! . Model Explanability . Let&#39;s look at what we were mostly interested. What are some best team compositions! . # from eli5.sklearn import PermutationImportance # perm = PermutationImportance(rnd_clf, random_state=42).fit(X_valid, y_valid) # eli5.show_weights(perm, feature_names=X_valid.columns.tolist()) # Will take billions years to compute . Let&#39;s try SHAP summary . import shap explainer = shap.TreeExplainer(rnd_clf) shap_values = explainer.shap_values(X_valid) shap.summary_plot(shap_values[1], X_valid) . We see that feature 0 (game length) tells us that the game favors blue team winning more when game is shorter which is unexpected. Note that it it not significant at all since SHAP value is -0.02 ~ 0.4 at most. | Generally, since all the values are 0 are 1, we can see clear 1-red and 0-blue (When it&#39;s 0 it has no impact on the prediction) | We can see feature 156(blue Mid Akali) helped RED team win more | Whereas Feature 462(red Top Tryndamere) helps the BLUE team win significantly more haha | From this chart, we can clearly see that each champion has very consistent and predictable contribution to their team&#39;s chance of winning | . Note that . 119 Kindred blue jg | 638 Caitlyn red adc | 60 Renekton blue top | 162 Cassiopeia blue mid | 535 Akali red mid | 376 Thresh blue support | 471 Volibear red top | 31 Jax blue top | 654 Kalista red adc | 290 Miss Fortune blue adc | 259 Ashe blue adc | 360 Rakan blue support | 210 Orianna blue mid | 462 Tryndamere red top | 445 Riven red top | 425 Lucian red top | 715 Janna red support | 156 Akali blue mid | 72 Sylas blue top | . Therefore our best teamp comp impacting positively on winning is ... . (Top)Renekton/Jax/Sylas (Jg)Kindred (Mid) Cassiopeia/Orianna (Adc)MF/Ashe (Sup)Thresh/Rakan | . Meanwhile worst team comp impacting negatively on winning is ... . (Top)Volibear/Trynd/Riven/Lucian (Mid)Akali (Adc)Caitlyn/Kalista (Sup)Janna | . We can also note that Jg role seem to not matter much.. : ) . def find_champ(i): temp_list = [99,54,101,73,57,96,52,102,68,52] for num in range(len(temp_list)): if (i-temp_list[num] &lt;= 0): return enc.categories_[num][i-1] else: i = i-temp_list[num] . # list_champ = [119, 638,60,162,535,376,471,31,654,290,259,360,210,462,445,425,715,156,72] # for champ in list_champ: # lane = &#39;&#39; # if champ &lt;= 99 or 385&lt;=champ&lt;=480: # lane = &quot;top&quot; # elif 100 &lt;= champ &lt;=153 or 481&lt;=champ&lt;=532: # lane = &quot;jg&quot; # elif 154 &lt;= champ &lt;=255 or 533&lt;=champ&lt;=634: # lane= &quot;mid&quot; # elif 256 &lt;= champ &lt;=327 or 635&lt;=champ&lt;=702: # lane= &quot;adc&quot; # else: # lane = &quot;support&quot; # team = &quot;blue&quot; if champ &lt;= 384 else &quot;red&quot; # print(champ, find_champ(champ), team, lane) #print(len(enc.categories_[0])) 99 #print(len(enc.categories_[1])) 54 //153 #print(len(enc.categories_[2])) 101 //254 #print(len(enc.categories_[3])) 73 //327 #print(len(enc.categories_[4])) 57 // UP TO 384 is blue team #print(len(enc.categories_[5])) 96 //480 #print(len(enc.categories_[6])) 52 //532 #print(len(enc.categories_[7])) 102 //634 #print(len(enc.categories_[8])) 68 //702 #print(len(enc.categories_[9])) 52 //754 . What if we didn&#39;t have game length, just champion compositions only? . X_1 = sparse_to_df . X_train_full, X_test, y_train_full, y_test = train_test_split(X_1,y,test_size=0.2, random_state=42) #len(X_train) = 3222 l = math.floor(3222*0.8) X_valid, X_train = X_train_full[:l], X_train_full[l:] y_valid, y_train = y_train_full[:l], y_train_full[l:] print(y_valid.shape) print(X_valid.shape) . (2577,) (2577, 754) . rnd_clf = RandomForestClassifier(n_estimators=2000, max_leaf_nodes=32, n_jobs=-1) rnd_clf.fit(X_train, y_train) . RandomForestClassifier(max_leaf_nodes=32, n_estimators=2000, n_jobs=-1) . y_val_pred = rnd_clf.predict(X_valid) val_acc = np.sum(y_val_pred == y_valid)/len(y_valid) print(&quot;validation accuracy: &quot;+str(val_acc)) . validation accuracy: 0.7691113698098564 . y_test_pred = rnd_clf.predict(X_test) test_acc = np.sum(y_test_pred == y_test)/len(y_test) print(&quot;test accuracy: &quot;+str(test_acc)) . test accuracy: 0.7692307692307693 . Surprisingly, accuracy only drops less than 0.01. We can conclude that planning out a team comp based on champion&#39;s strength on early vs late game does not help win more. This can be explained by an example. Let&#39;s say I picked kayle which is the best late game champion. We may win games with longer duration more but will lose more short games due to her weakness early. So the overall win rate balances out. . Conclusion . best: (Top)Renekton/Jax/Sylas (Jg)Kindred (Mid) Cassiopeia/Orianna (Adc)MF/Ashe (Sup)Thresh/Rakan | worst: (Top)Volibear/Trynd/Riven/Lucian (Mid)Akali (Adc)Caitlyn/Kalista (Sup)Janna | . We know that in the world of solo queue, picking the above champions will not gurantee a win. Sometimes people are autofilled, meaning they aren&#39;t playing on their best role. People may disconnect, resulting in games favoring the opposite team. There are too many unknown factors like this, making it impossible to predict 100% of the game outcomes correctly. . As a former high elo NA player myself, I can say that generally, the &#39;best team&#39; above have champions that doesn&#39;t get countered too often and is a good pick into anything. (This may not be the case for top because I&#39;ve never really cared about top lanes as a support player :). But for &#39;worst team&#39; champions, they are often easily countered. (Especially bottom lane) . The biggest surprise was blue team wins more early and red team wins more late (Very slightly but certainly) for some reason. Also jg mattering the least was a surprise as well. .",
            "url": "https://leejaeka.github.io/fastblog/fastpages/jupyter/2020/10/28/lolpredict.html",
            "relUrl": "/fastpages/jupyter/2020/10/28/lolpredict.html",
            "date": " • Oct 28, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Renekton Croc plush detector",
            "content": "import os key = os.environ.get(&#39;AZURE_SEARCH_KEY&#39;, &#39;1a25802a09ab45e4a082267f88ee5bd1&#39;) . reptile_types = &#39;crocodile&#39;,&#39;alligator plush&#39;, &#39;renekton&#39; path = Path(&#39;reptiles&#39;) . if not path.exists(): path.mkdir() for o in reptile_types: dest = (path/o) dest.mkdir(exist_ok=True) results = search_images_bing(key, f&#39;{o}&#39;) download_images(dest, urls=results.attrgot(&#39;content_url&#39;)) . fns = get_image_files(path) fns . (#416) [Path(&#39;reptiles/alligator plush/00000000.jpg&#39;),Path(&#39;reptiles/alligator plush/00000001.jpg&#39;),Path(&#39;reptiles/alligator plush/00000002.jpg&#39;),Path(&#39;reptiles/alligator plush/00000003.jpg&#39;),Path(&#39;reptiles/alligator plush/00000004.jpeg&#39;),Path(&#39;reptiles/alligator plush/00000005.jpg&#39;),Path(&#39;reptiles/alligator plush/00000006.jpg&#39;),Path(&#39;reptiles/alligator plush/00000007.jpg&#39;),Path(&#39;reptiles/alligator plush/00000008.jpg&#39;),Path(&#39;reptiles/alligator plush/00000009.jpg&#39;)...] . failed = verify_images(fns) failed.map(Path.unlink) . (#0) [] . class DataLoaders(GetAttr): num_workers=0 def __init__(self, *loaders): self.loaders = loaders def __getitem__(self, i): return self.loaders[i] train,valid = add_props(lambda i, self: self[i]) . reptiles = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(128)) . dls = reptiles.dataloaders(path) dls.valid.show_batch(max_n=4, nrows=1) . reptiles = reptiles.new(item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) dls = reptiles.dataloaders(path, num_workers=0) # &lt;- num_workers=0 to prevent window error . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . epoch train_loss valid_loss error_rate time . 0 | 1.455003 | 0.249432 | 0.084337 | 00:18 | . epoch train_loss valid_loss error_rate time . 0 | 0.104521 | 0.049535 | 0.024096 | 00:18 | . 1 | 0.068319 | 0.012980 | 0.012048 | 00:18 | . 2 | 0.052283 | 0.011862 | 0.000000 | 00:19 | . 3 | 0.041685 | 0.010840 | 0.000000 | 00:19 | . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . Result is very good . # cleaner # for idx in cleaner.delete(): cleaner.fns[idx].unlink() # delete . Let&#39;s test . my_renek = PILImage.create(&quot;renek_plush.png&quot;) display(my_renek.to_thumb(256,256)) . pred, pred_idx, probs =learn.predict(my_renek) f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; . &#39;Prediction: alligator plush; Probability: 0.9391&#39; . Very good. It is very accurate since my drawing of a plush is very realistic. . renek = PILImage.create(&quot;renek_test.png&quot;) display(renek.to_thumb(256,256)) pred, pred_idx, probs =learn.predict(renek) f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; . &#39;Prediction: renekton; Probability: 0.9834&#39; . Easily recognizes my drawing of Renekton as well. I guess I&#39;m an artist . renek_withoutbg = PILImage.create(&quot;renek_test1.png&quot;) display(renek_withoutbg.to_thumb(256,256)) pred, pred_idx, probs =learn.predict(renek_withoutbg) f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; . &#39;Prediction: renekton; Probability: 0.9674&#39; . Expected the model to predict plush becasue I removed the background but it&#39;s too smart. (In dataset a lot of plush had empty white background contrast to lots of Renekton images having dark backgrounds) . beard = PILImage.create(&quot;beard.jpg&quot;) display(beard.to_thumb(200,200)) pred, pred_idx, probs =learn.predict(beard) f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; . &#39;Prediction: alligator plush; Probability: 0.8644&#39; . Indeed I am an alligator plush with my fake beard! . learn.export() . RUN CODE BELOW TO MAKE YOUR OWN TEST (CLICK UPLOAD) . from fastai.vision.widgets import * btn_upload = widgets.FileUpload() out_pl = widgets.Output() lbl_pred = widgets.Label() . path = Path(&#39;&#39;) learn_inf = load_learner(path/&#39;export.pkl&#39;, cpu=True) . def on_data_change(change): lbl_pred.value = &#39;&#39; img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) pred,pred_idx,probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; . btn_upload.observe(on_data_change, names=[&#39;data&#39;]) display(VBox([widgets.Label(&#39;Feed me a reptile photo!&#39;), btn_upload, out_pl, lbl_pred])) .",
            "url": "https://leejaeka.github.io/fastblog/2020/10/27/croc.html",
            "relUrl": "/2020/10/27/croc.html",
            "date": " • Oct 27, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "City Detector",
            "content": "Let&#39;s make app that recognizes Coquitlam! . import os key = os.environ.get(&#39;AZURE_SEARCH_KEY&#39;, &#39;39f25aae8d744a528b964a94a4af8b58&#39;) . Let&#39;s get the images of each city . city_types = &#39;seoul city&#39;,&#39;coquitlam&#39;,&#39;paris city&#39;, &#39;new york city&#39; path = Path(&#39;cities&#39;) . if not path.exists(): path.mkdir() for o in city_types: dest = (path/o) dest.mkdir(exist_ok=True) results = search_images_bing(key, f&#39;{o}&#39;) download_images(dest, urls=results.attrgot(&#39;content_url&#39;)) . fns = get_image_files(path) fns . (#596) [Path(&#39;cities/coquitlam/00000000.jpg&#39;),Path(&#39;cities/coquitlam/00000001.jpg&#39;),Path(&#39;cities/coquitlam/00000002.png&#39;),Path(&#39;cities/coquitlam/00000003.jpg&#39;),Path(&#39;cities/coquitlam/00000004.jpg&#39;),Path(&#39;cities/coquitlam/00000005.jpg&#39;),Path(&#39;cities/coquitlam/00000006.jpg&#39;),Path(&#39;cities/coquitlam/00000007.jpg&#39;),Path(&#39;cities/coquitlam/00000008.jpg&#39;),Path(&#39;cities/coquitlam/00000009.jpg&#39;)...] . failed = verify_images(fns) failed . (#16) [Path(&#39;cities/coquitlam/00000067.jpg&#39;),Path(&#39;cities/coquitlam/00000077.JPG&#39;),Path(&#39;cities/coquitlam/00000079.jpg&#39;),Path(&#39;cities/coquitlam/00000135.jpg&#39;),Path(&#39;cities/new york city/00000010.jpg&#39;),Path(&#39;cities/new york city/00000014.jpg&#39;),Path(&#39;cities/new york city/00000020.jpg&#39;),Path(&#39;cities/new york city/00000026.jpg&#39;),Path(&#39;cities/new york city/00000029.jpg&#39;),Path(&#39;cities/new york city/00000037.jpg&#39;)...] . failed.map(Path.unlink) . (#16) [None,None,None,None,None,None,None,None,None,None...] . Let&#39;s load the data . class DataLoaders(GetAttr): def __init__(self, *loaders): self.loaders = loaders def __getitem__(self, i): return self.loaders[i] train,valid = add_props(lambda i, self: self[i]) . cities = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(128)) . dls = cities.dataloaders(path) dls.valid.show_batch(max_n=8, nrows = 2) . Let&#39;s scale and augment the datas . cities = cities.new(item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) dls = cities.dataloaders(path, num_workers=0) # &lt;- num_workers=0 to prevent window error . Let&#39;s build and run a CNN model . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . epoch train_loss valid_loss error_rate time . 0 | 2.187395 | 1.315027 | 0.482759 | 00:42 | . epoch train_loss valid_loss error_rate time . 0 | 1.307873 | 0.871226 | 0.336207 | 00:41 | . 1 | 1.064780 | 0.831430 | 0.241379 | 00:41 | . 2 | 0.876646 | 0.767134 | 0.215517 | 00:41 | . 3 | 0.784991 | 0.738216 | 0.224138 | 00:49 | . Let&#39;s look at the confusion matrix . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . We got an accuracy of 26/90 = 71% (rounded) . interp.plot_top_losses(2,nrows=2) . Let&#39;s try to clean up the dataset . cleaner = ImageClassifierCleaner(learn) cleaner . for idx in cleaner.delete(): cleaner.fns[idx].unlink() # delete . dls = cities.dataloaders(path, num_workers=0) learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(5) . epoch train_loss valid_loss error_rate time . 0 | 1.933403 | 1.322945 | 0.460870 | 00:38 | . epoch train_loss valid_loss error_rate time . 0 | 1.258363 | 0.800413 | 0.347826 | 00:37 | . 1 | 1.014135 | 0.660854 | 0.243478 | 00:38 | . 2 | 0.851025 | 0.609896 | 0.243478 | 00:38 | . 3 | 0.725140 | 0.591347 | 0.217391 | 00:37 | . 4 | 0.623130 | 0.582418 | 0.226087 | 00:37 | . Valid_loss doesn&#39;t decrease so we stop . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . interp.plot_top_losses(6,nrows=2) . It looks like it has a hard time highlighting seoul city&#39;s characteristics as most error comes from seoul images. Suspected factors include seoul having new york like buildings, mountains like coquitlam and brick structures like paris city. . Ignore Below (Deployment IPR) . learn.export() . path = Path() path.ls(file_exts=&#39;.pkl&#39;) . (#1) [Path(&#39;export.pkl&#39;)] . learn_inf = load_learner(path/&#39;export.pkl&#39;) . learn_inf.dls.vocab . [&#39;coquitlam&#39;, &#39;new york city&#39;, &#39;paris city&#39;, &#39;seoul city&#39;] . btn_upload = widgets.FileUpload() btn_upload . img = PILImage.create(btn_upload.data[-1]) . out_pl = widgets.Output() out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) out_pl . pred,pred_idx,probs = learn_inf.predict(img) #hide_output lbl_pred = widgets.Label() lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; lbl_pred . Classify Button &amp; Event Handler . btn_run = widgets.Button(description=&#39;Classify&#39;) btn_run . def on_click_classify(change): img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) pred,pred_idx,probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; btn_run.on_click(on_click_classify) .",
            "url": "https://leejaeka.github.io/fastblog/fastpages/jupyter/2020/10/27/city.html",
            "relUrl": "/fastpages/jupyter/2020/10/27/city.html",
            "date": " • Oct 27, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Reptile Detector",
            "content": "import os key = os.environ.get(&#39;AZURE_SEARCH_KEY&#39;, &#39;Key omitted for security&#39;) . reptile_types = &#39;crocodile&#39;,&#39;alligator plushies&#39;, &#39;renekton&#39; path = Path(&#39;reptiles&#39;) . # path.mkdir() # for o in reptile_types: # dest = (path/o) # dest.mkdir(exist_ok=True) # results = search_images_bing(key, f&#39;{o}&#39;) # download_images(dest, urls=results.attrgot(&#39;content_url&#39;)) . . 73.33% [110/150 00:32&lt;00:11] fns = get_image_files(path) fns . (#444) [Path(&#39;reptiles/alligator/00000000.jpg&#39;),Path(&#39;reptiles/alligator/00000001.jpg&#39;),Path(&#39;reptiles/alligator/00000002.jpg&#39;),Path(&#39;reptiles/alligator/00000003.jpg&#39;),Path(&#39;reptiles/alligator/00000004.jpg&#39;),Path(&#39;reptiles/alligator/00000005.jpg&#39;),Path(&#39;reptiles/alligator/00000006.jpg&#39;),Path(&#39;reptiles/alligator/00000007.jpg&#39;),Path(&#39;reptiles/alligator/00000008.jpg&#39;),Path(&#39;reptiles/alligator/00000009.jpg&#39;)...] . failed = verify_images(fns) failed . (#7) [Path(&#39;reptiles/alligator/00000002.jpg&#39;),Path(&#39;reptiles/caiman/00000021.jpg&#39;),Path(&#39;reptiles/caiman/00000071.jpg&#39;),Path(&#39;reptiles/caiman/00000115.JPG&#39;),Path(&#39;reptiles/crocodile/00000017.jpg&#39;),Path(&#39;reptiles/crocodile/00000036.jpg&#39;),Path(&#39;reptiles/crocodile/00000089.jpg&#39;)] . failed.map(Path.unlink) . (#7) [None,None,None,None,None,None,None] . class DataLoaders(GetAttr): num_workers=0 def __init__(self, *loaders): self.loaders = loaders def __getitem__(self, i): return self.loaders[i] train,valid = add_props(lambda i, self: self[i]) . reptiles = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(128)) . dls = reptiles.dataloaders(path) . dls.valid.show_batch(max_n=4, nrows=1) . reptiles = reptiles.new(item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) dls = reptiles.dataloaders(path, num_workers=0) # &lt;- num_workers=0 to prevent window error . dls.train.show_batch(max_n=8,nrows=2,unique=True) . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . epoch train_loss valid_loss error_rate time . 0 | 1.970919 | 1.043408 | 0.436782 | 00:26 | . D: anaconda3 lib site-packages PIL Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images warnings.warn( . epoch train_loss valid_loss error_rate time . 0 | 1.344288 | 0.993782 | 0.413793 | 00:25 | . 1 | 1.237567 | 1.008499 | 0.402299 | 00:25 | . 2 | 1.032747 | 0.994638 | 0.333333 | 00:25 | . 3 | 0.935045 | 0.998598 | 0.321839 | 00:25 | . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . interp.plot_top_losses(3,nrows=2) . cleaner = ImageClassifierCleaner(learn) cleaner . FileNotFoundError Traceback (most recent call last) &lt;ipython-input-61-1df86057f98d&gt; in &lt;module&gt; -&gt; 1 cleaner = ImageClassifierCleaner(learn) 2 cleaner D: anaconda3 lib site-packages fastai vision widgets.py in __init__(self, learn, **kwargs) 79 self.dd_cats = Dropdown(options=vocab) 80 self.dd_ds = Dropdown(options=(&#39;Train&#39;,&#39;Valid&#39;)) &gt; 81 self.iwis = _get_iw_info(learn,0),_get_iw_info(learn,1) 82 self.dd_ds.observe(self.on_change_ds, &#39;value&#39;) 83 self.dd_cats.observe(self.on_change_ds, &#39;value&#39;) D: anaconda3 lib site-packages fastai vision widgets.py in _get_iw_info(learn, ds_idx) 66 def _get_iw_info(learn, ds_idx=0): 67 dl = learn.dls[ds_idx].new(shuffle=False, drop_last=False) &gt; 68 inp,probs,targs,preds,losses = learn.get_preds(dl=dl, with_input=True, with_loss=True, with_decoded=True) 69 inp,targs = L(zip(*dl.decode_batch((inp,targs), max_n=9999))) 70 return L([dl.dataset.items,targs,losses]).zip() D: anaconda3 lib site-packages fastai learner.py in get_preds(self, ds_idx, dl, with_input, with_decoded, with_loss, act, inner, reorder, cbs, **kwargs) 233 if with_loss: ctx_mgrs.append(self.loss_not_reduced()) 234 with ContextManagers(ctx_mgrs): --&gt; 235 self._do_epoch_validate(dl=dl) 236 if act is None: act = getattr(self.loss_func, &#39;activation&#39;, noop) 237 res = cb.all_tensors() D: anaconda3 lib site-packages fastai learner.py in _do_epoch_validate(self, ds_idx, dl) 186 if dl is None: dl = self.dls[ds_idx] 187 self.dl = dl --&gt; 188 with torch.no_grad(): self._with_events(self.all_batches, &#39;validate&#39;, CancelValidException) 189 190 def _do_epoch(self): D: anaconda3 lib site-packages fastai learner.py in _with_events(self, f, event_type, ex, final) 153 154 def _with_events(self, f, event_type, ex, final=noop): --&gt; 155 try: self(f&#39;before_{event_type}&#39;) ;f() 156 except ex: self(f&#39;after_cancel_{event_type}&#39;) 157 finally: self(f&#39;after_{event_type}&#39;) ;final() D: anaconda3 lib site-packages fastai learner.py in all_batches(self) 159 def all_batches(self): 160 self.n_iter = len(self.dl) --&gt; 161 for o in enumerate(self.dl): self.one_batch(*o) 162 163 def _do_one_batch(self): D: anaconda3 lib site-packages fastai data load.py in __iter__(self) 100 self.before_iter() 101 self.__idxs=self.get_idxs() # called in context of main process (not workers/subprocesses) --&gt; 102 for b in _loaders[self.fake_l.num_workers==0](self.fake_l): 103 if self.device is not None: b = to_device(b, self.device) 104 yield self.after_batch(b) D: anaconda3 lib site-packages torch utils data dataloader.py in __next__(self) 361 362 def __next__(self): --&gt; 363 data = self._next_data() 364 self._num_yielded += 1 365 if self._dataset_kind == _DatasetKind.Iterable and D: anaconda3 lib site-packages torch utils data dataloader.py in _next_data(self) 401 def _next_data(self): 402 index = self._next_index() # may raise StopIteration --&gt; 403 data = self._dataset_fetcher.fetch(index) # may raise StopIteration 404 if self._pin_memory: 405 data = _utils.pin_memory.pin_memory(data) D: anaconda3 lib site-packages torch utils data _utils fetch.py in fetch(self, possibly_batched_index) 32 raise StopIteration 33 else: &gt; 34 data = next(self.dataset_iter) 35 return self.collate_fn(data) 36 D: anaconda3 lib site-packages fastai data load.py in create_batches(self, samps) 109 self.it = iter(self.dataset) if self.dataset is not None else None 110 res = filter(lambda o:o is not None, map(self.do_item, samps)) --&gt; 111 yield from map(self.do_batch, self.chunkify(res)) 112 113 def new(self, dataset=None, cls=None, **kwargs): D: anaconda3 lib site-packages fastcore utils.py in chunked(it, chunk_sz, drop_last, n_chunks) 164 if not isinstance(it, Iterator): it = iter(it) 165 while True: --&gt; 166 res = list(itertools.islice(it, chunk_sz)) 167 if res and (len(res)==chunk_sz or not drop_last): yield res 168 if len(res)&lt;chunk_sz: return D: anaconda3 lib site-packages fastai data load.py in do_item(self, s) 122 def prebatched(self): return self.bs is None 123 def do_item(self, s): --&gt; 124 try: return self.after_item(self.create_item(s)) 125 except SkipItemException: return None 126 def chunkify(self, b): return b if self.prebatched else chunked(b, self.bs, self.drop_last) D: anaconda3 lib site-packages fastai data load.py in create_item(self, s) 128 def randomize(self): self.rng = random.Random(self.rng.randint(0,2**32-1)) 129 def retain(self, res, b): return retain_types(res, b[0] if is_listy(b) else b) --&gt; 130 def create_item(self, s): return next(self.it) if s is None else self.dataset[s] 131 def create_batch(self, b): return (fa_collate,fa_convert)[self.prebatched](b) 132 def do_batch(self, b): return self.retain(self.create_batch(self.before_batch(b)), b) D: anaconda3 lib site-packages fastai data core.py in __getitem__(self, it) 312 313 def __getitem__(self, it): --&gt; 314 res = tuple([tl[it] for tl in self.tls]) 315 return res if is_indexer(it) else list(zip(*res)) 316 D: anaconda3 lib site-packages fastai data core.py in &lt;listcomp&gt;(.0) 312 313 def __getitem__(self, it): --&gt; 314 res = tuple([tl[it] for tl in self.tls]) 315 return res if is_indexer(it) else list(zip(*res)) 316 D: anaconda3 lib site-packages fastai data core.py in __getitem__(self, idx) 278 res = super().__getitem__(idx) 279 if self._after_item is None: return res --&gt; 280 return self._after_item(res) if is_indexer(idx) else res.map(self._after_item) 281 282 # Cell D: anaconda3 lib site-packages fastai data core.py in _after_item(self, o) 240 return super()._new(items, tfms=self.tfms, do_setup=False, types=self.types, split_idx=split_idx, **kwargs) 241 def subset(self, i): return self._new(self._get(self.splits[i]), split_idx=i) --&gt; 242 def _after_item(self, o): return self.tfms(o) 243 def __repr__(self): return f&#34;{self.__class__.__name__}: {self.items} ntfms - {self.tfms.fs}&#34; 244 def __iter__(self): return (self[i] for i in range(len(self))) D: anaconda3 lib site-packages fastcore transform.py in __call__(self, o) 196 self.fs.append(t) 197 --&gt; 198 def __call__(self, o): return compose_tfms(o, tfms=self.fs, split_idx=self.split_idx) 199 def __repr__(self): return f&#34;Pipeline: {&#39; -&gt; &#39;.join([f.name for f in self.fs if f.name != &#39;noop&#39;])}&#34; 200 def __getitem__(self,i): return self.fs[i] D: anaconda3 lib site-packages fastcore transform.py in compose_tfms(x, tfms, is_enc, reverse, **kwargs) 148 for f in tfms: 149 if not is_enc: f = f.decode --&gt; 150 x = f(x, **kwargs) 151 return x 152 D: anaconda3 lib site-packages fastcore transform.py in __call__(self, x, **kwargs) 71 @property 72 def name(self): return getattr(self, &#39;_name&#39;, _get_name(self)) &gt; 73 def __call__(self, x, **kwargs): return self._call(&#39;encodes&#39;, x, **kwargs) 74 def decode (self, x, **kwargs): return self._call(&#39;decodes&#39;, x, **kwargs) 75 def __repr__(self): return f&#39;{self.name}: nencodes: {self.encodes}decodes: {self.decodes}&#39; D: anaconda3 lib site-packages fastcore transform.py in _call(self, fn, x, split_idx, **kwargs) 81 def _call(self, fn, x, split_idx=None, **kwargs): 82 if split_idx!=self.split_idx and self.split_idx is not None: return x &gt; 83 return self._do_call(getattr(self, fn), x, **kwargs) 84 85 def _do_call(self, f, x, **kwargs): D: anaconda3 lib site-packages fastcore transform.py in _do_call(self, f, x, **kwargs) 87 if f is None: return x 88 ret = f.returns_none(x) if hasattr(f,&#39;returns_none&#39;) else None &gt; 89 return retain_type(f(x, **kwargs), x, ret) 90 res = tuple(self._do_call(f, x_, **kwargs) for x_ in x) 91 return retain_type(res, x) D: anaconda3 lib site-packages fastcore dispatch.py in __call__(self, *args, **kwargs) 127 elif self.inst is not None: f = MethodType(f, self.inst) 128 elif self.owner is not None: f = MethodType(f, self.owner) --&gt; 129 return f(*args, **kwargs) 130 131 def __get__(self, inst, owner): D: anaconda3 lib site-packages fastai vision core.py in create(cls, fn, **kwargs) 108 if isinstance(fn,ndarray): return cls(Image.fromarray(fn)) 109 if isinstance(fn,bytes): fn = io.BytesIO(fn) --&gt; 110 return cls(load_image(fn, **merge(cls._open_args, kwargs))) 111 112 def show(self, ctx=None, **kwargs): D: anaconda3 lib site-packages fastai vision core.py in load_image(fn, mode) 83 def load_image(fn, mode=None): 84 &#34;Open and load a `PIL.Image` and convert to `mode`&#34; &gt; 85 im = Image.open(fn) 86 im.load() 87 im = im._new(im.im) D: anaconda3 lib site-packages PIL Image.py in open(fp, mode) 2876 2877 if filename: -&gt; 2878 fp = builtins.open(filename, &#34;rb&#34;) 2879 exclusive_fp = True 2880 FileNotFoundError: [Errno 2] No such file or directory: &#39;D: Project Reptile Detector Croc-Alligator-Caiman-Detector reptiles alligator 00000099.jpg&#39; . for idx in cleaner.delete(): cleaner.fns[idx].unlink() # delete . FileNotFoundError Traceback (most recent call last) &lt;ipython-input-59-d0ac79185ce3&gt; in &lt;module&gt; -&gt; 1 for idx in cleaner.delete(): cleaner.fns[idx].unlink() # delete D: anaconda3 lib pathlib.py in unlink(self, missing_ok) 1319 self._raise_closed() 1320 try: -&gt; 1321 self._accessor.unlink(self) 1322 except FileNotFoundError: 1323 if not missing_ok: FileNotFoundError: [WinError 2] The system cannot find the file specified: &#39;reptiles crocodile 00000102.jpg&#39; . for idx, cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat) . reptiles = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(128)) . reptiles = reptiles.new(item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) dls = reptiles.dataloaders(path, num_workers=0) # &lt;- num_workers=0 to prevent window error . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . epoch train_loss valid_loss error_rate time . 0 | 2.024061 | 1.739658 | 0.593023 | 00:23 | . epoch train_loss valid_loss error_rate time . 0 | 1.284611 | 1.220839 | 0.500000 | 00:23 | . 1 | 1.172451 | 1.031749 | 0.348837 | 00:24 | . 2 | 1.023675 | 1.042305 | 0.313953 | 00:24 | . 3 | 0.893127 | 1.034520 | 0.325581 | 00:24 | . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . A little bit better! But still horrible! . Conclusion . I am not a reptile expert so I cannot tell the difference alligators and crocodiles unless their snouts are clearly shown from top-down view.. Implying that I cannot clean up the messy data efficiently. One solution is to gather more and better data but I&#39;m not that interested in this topic anymore so I will stop here. Honestly, I&#39;m surprised the CNN was able to somewhat tell with the given datas. .",
            "url": "https://leejaeka.github.io/fastblog/fastpages/jupyter/2020/10/26/reptile.html",
            "relUrl": "/fastpages/jupyter/2020/10/26/reptile.html",
            "date": " • Oct 26, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://leejaeka.github.io/fastblog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://leejaeka.github.io/fastblog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Yes it is I! .",
          "url": "https://leejaeka.github.io/fastblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://leejaeka.github.io/fastblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}