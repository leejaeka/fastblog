{
  
    
        "post0": {
            "title": "omg",
            "content": "",
            "url": "https://leejaeka.github.io/jaekangai/2021/04/23/omg.html",
            "relUrl": "/2021/04/23/omg.html",
            "date": " ‚Ä¢ Apr 23, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Fairness in Hiring and Salary Statistical Report ‚öñÔ∏è",
            "content": "Full Report . https://github.com/leejaeka/black_saber_statistic_reporting . Here are some sneak peaks! .",
            "url": "https://leejaeka.github.io/jaekangai/fastpages/r/glmer/data_exploration/data%20wrangling/report/2021/04/23/FairnessReport.html",
            "relUrl": "/fastpages/r/glmer/data_exploration/data%20wrangling/report/2021/04/23/FairnessReport.html",
            "date": " ‚Ä¢ Apr 23, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Book Reviews and Storytime üìö",
            "content": "Book Reviews . The simplicity and the colorful cover made me decide to buy this book. I am glad that I purchased this book because as a statistic student this book showed me the real world statistics that textbooks cannot show with their equations and proofs. My favorite story was about a killer disguised as a doctor who killed tens of innocent seniors and his suspicions were confirmed with statistical methods. Every chapter starts with a really interesting question, for example, &quot;Who was the luckiest person on Titanic?&quot; and answers with a statistical approach. It was delightful seeing concepts like p-value, hypothesis testing, regression and inference playing in real life problems, sometimes comically failing. . I would definitely recommend this book to anyone wondering &quot;I know statistics are useful but where do we see these being used?&quot;. Personal rating: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê . This was my first ever machine learning book that I bought because I suddenly got really interested in data science. I remember reading this book everyday and getting excited when I copied-pasted the algorithms shown in the book and they worked. The book was definetly beginner friendly in the beginning but it becomes really challenging in the later chapters. For example reading about attention mechanism in recurrent neural networks. (I am still confused to this day). The book really shines because it explains everything with pictures and diagrams. Also note they have a Git repository for the book so you can quickly use any machine learning you learn in real life. . I would definetly recommen this book to anyone wanting to learn about machine learning. I wouldn&#39;t rely just on the book though, it will be really helpful to take course or other similiar books as well because I find that learning about machine learning is best when you just keep reading and practicing more. Personal rating: ‚≠ê‚≠ê‚≠ê‚≠ê . O&#39;Reilly always produce pretty cover that it is hard to resist not buying their book. I got interested in generative deep learning because it is one of the big machine learning pieces(generative learning, unsupervised learning, supervised learning, reinforcement learning). When I was reading this book, I really felt the author really cares about this book and that they took a lot of effort writing this book. I loved its way of explaining deep learning concepts through simple pictures and stories. For example, it compares encoders and decoders as a painter trying to fool a museum owner making him think that the painting is real when it is really fake. . Overall, this book does not answer every question you have since generative deep learning is fairly unexplored area but if you want to quickly learn enough to get yourself started inplementing genrative deep learning models, this is a great book. Personal rating: ‚≠ê‚≠ê‚≠ê‚≠ê . Kaggle is a great data science online community created by Google. They often hold highly competitive global data science competitions where a small data scientist like myself gain huge amount of experience, often by consuming tiny crumbles of informations and practices from experts. This book was written by one of the biggest giant in the community who have won multiple competitions. What the book does the best is guiding readers creating their own data science projects. If you ever get stuck in one area, the book gives you clear directions. If you are a programmer interested in writing machine learning applications, this is a great guide. Personal rating: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê . Storytime . 1. Monte Hall Problem . In this 1963 American tv show, the host Monty Hall proposes a game to the contestant. There are three closed doors. Two of the doors have goats behind them and one of the doors has a fancy car behind it. The contestants get to keep whatever is behind the door they choose. Once the contestant chooses one of the doors, Monty opens one of the unchosen doors showing a goat behind it. Then Monty asks the contestant if he/she wants to change their decision on the remaining two doors. Should the contestant change? . Most people will think the contestant has 1/2 chance of winning. I too thought this way but by switching, the contestant actually doubles their chance of winning. To see this most clearly, consider a similar test with 100 doors instead of 3. Now, once you choose a door, Monty opens 98 other doors showing goats and asks you if you want to switch. Obviously, you should switch to the other door because the original choice had 1/100 chance of winning but the other door is same as having had 99 choices to choose the correct door with 99% chance of winning. So in our original case, the door we chose initially has 1/3 chance of winning but choosing the other door has 2/3 chance. Hence doubling our chance of winning. If you are still not convinced, you can try out yourself here http://www.shodor.org/interactivate/activities/SimpleMontyHall/. . Reference . Books: . Naked Statistics: Stripping the Dread from the Data Book(2012) by Charles Wheelan | .",
            "url": "https://leejaeka.github.io/jaekangai/fastpages/jupyter/meme/book_review/story/2021/04/04/Short-Storytime.html",
            "relUrl": "/fastpages/jupyter/meme/book_review/story/2021/04/04/Short-Storytime.html",
            "date": " ‚Ä¢ Apr 4, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Jane Street Market Prediction üéØ",
            "content": "Got a score of 9443.499 (249th place out of 3616 competitors) using MLP. . Library &#128194; . import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation from tensorflow.keras.models import Model, Sequential from tensorflow.keras.losses import BinaryCrossentropy from tensorflow.keras.optimizers import Adam from tensorflow.keras.callbacks import EarlyStopping from tensorflow.keras.layers.experimental.preprocessing import Normalization import tensorflow as tf import matplotlib.pyplot as plt from tqdm import tqdm import seaborn as sns from random import choices !pip install datatable &gt; /dev/null import datatable as dt from sklearn import impute import gc import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) SEED = 42 tf.random.set_seed(SEED) np.random.seed(SEED) . Methodology . Null Values &#127539; . As discussed before in my EDA notebook, we have couple of options to handle null values. . Drop all nans | Impute with median or mean | Feedforward/backward | KNN imputer | Be creative! | In this notebook, I used KNN imputer with 5 nearest neighbors to fill the nans. This takes a long time to run so I suggest downloading the imputed data files from here by louise2001. Note that he also uploaded soft and iterative imputes. . Import Data &#128218; . In this notebook, we are just going to load the imputed data instead of running the feature engineering here. Since it is very time consuming and takes a lot of RAM. . imputed_df = dt.fread(&#39;../input/data-wrangling/imputed.csv&#39;) imputed_df = imputed_df.to_pandas() train = dt.fread(&#39;../input/data-wrangling/one_on_top.csv&#39;) train = train.to_pandas() df = pd.concat([train, imputed_df], axis=1, ignore_index=False) del train, imputed_df gc.collect() . date weight ts_id resp_1 resp_2 resp_3 resp resp_4 feature_0 feature_1 ... feature_120 feature_121 feature_122 feature_123 feature_124 feature_125 feature_126 feature_127 feature_128 feature_129 . 0 0 | 0.000000 | 0 | 0.009916 | 0.014079 | 0.008773 | 0.006270 | 0.001390 | 1 | -1.872746 | ... | 0.603878 | 6.086305 | 1.168391 | 8.313583 | 1.782433 | 14.018213 | 2.653056 | 12.600292 | 2.301488 | 11.445807 | . 1 0 | 0.138531 | 4 | 0.001252 | 0.002165 | -0.001215 | -0.002604 | -0.006219 | 1 | -3.172026 | ... | 0.745019 | 5.354213 | 0.344850 | 4.101145 | 0.614252 | 6.623456 | 0.800129 | 5.233243 | 0.362636 | 3.926633 | . 2 0 | 0.116557 | 8 | -0.005460 | -0.007301 | -0.009085 | -0.001677 | -0.003546 | 1 | -3.172026 | ... | 1.120067 | 4.167835 | 1.537913 | 4.785838 | 1.637435 | 6.968002 | 2.354338 | 5.825499 | 1.778029 | 4.740577 | . 3 0 | 0.160117 | 9 | 0.005976 | 0.004345 | 0.023712 | 0.020317 | 0.035360 | 1 | 2.744408 | ... | 1.430190 | 3.332330 | 1.796860 | 3.177064 | 0.999252 | 2.906432 | 1.589816 | 2.435999 | 1.472419 | 2.245991 | . 4 0 | 0.109651 | 10 | 0.006899 | 0.003405 | 0.000134 | -0.000690 | -0.003040 | 1 | -3.172026 | ... | 1.581096 | 6.305170 | 2.324290 | 4.881133 | 2.115830 | 6.337250 | 3.059392 | 5.350729 | 2.755876 | 4.968388 | . 5 rows √ó 138 columns . Feature Engineering &#128295; . We first do two feature engineering right off the bat. . We are going to drop any rows with &#39;weight&#39; column equal to 0. This tells us that overall gain from such trade is 0. This would be like telling machine to just guess if learned correctly. | To explain why we are dropping all dates before day 85 can be shown visually below. Before the day 85, we can clearly see that the trend has changed quite drastically. | 7310 . df = df.query(&#39;date &gt; 85&#39;).reset_index(drop = True) df = df[df[&#39;weight&#39;] != 0] . Note that we only have 130 features compared to over 2 million datas. We easily make more features and avoid curse of dimensionality. . # Add action column (this is our target) df[&#39;action&#39;] = ((df[&#39;resp&#39;].values) &gt; 0).astype(int) # feature names features = [c for c in df.columns if &quot;feature&quot; in c] # resp names resp_cols = [&#39;resp_1&#39;, &#39;resp_2&#39;, &#39;resp_3&#39;, &#39;resp&#39;, &#39;resp_4&#39;] . df = df.loc[:, df.columns.str.contains(&#39;feature|resp&#39;, regex=True)] . Let us do log transform and add them as new columns to the dataframe. Since performing on all features will give me out of memory error, let&#39;s do this on group_0 which has tag_0 from features.csv. For more information, check out my EDA notebook. . tag_0_group = [9, 10, 19, 20, 29, 30, 56, 73, 79, 85, 91, 97, 103, 109, 115, 122, 123] for col in tag_0_group: df[str(&#39;log_&#39;+str(col))] = (df[str(&#39;feature_&#39;+str(col))]-df[str(&#39;feature_&#39;+str(col))].min()+1).transform(np.log) . feature_0 feature_1 feature_2 feature_3 feature_4 feature_5 feature_6 feature_7 feature_8 feature_9 ... log_73 log_79 log_85 log_91 log_97 log_103 log_109 log_115 log_122 log_123 . 0 1 | 3.151305 | 5.467693 | -0.164505 | -0.189219 | 0.663966 | 0.988896 | 0.661407 | 0.897346 | 2.184804 | ... | 4.371497 | 4.954968 | 1.009198e-07 | 1.235292e-07 | 1.372731e+00 | 7.735990e-01 | 1.583237 | 0.994426 | 2.206237 | 2.390646 | . 2 1 | 1.514607 | 0.596214 | 0.324062 | 0.154730 | 0.845069 | 0.521491 | 0.860309 | 0.595352 | 0.310387 | ... | 4.385074 | 4.956836 | 1.009198e-07 | 1.235292e-07 | 7.875868e-01 | 5.235099e-01 | 0.793093 | 0.487668 | 2.191892 | 2.100277 | . 4 1 | -0.833827 | -0.049648 | 0.262484 | 0.421901 | 0.098124 | 0.171741 | 0.034455 | 0.169169 | 0.512029 | ... | 4.373749 | 4.934122 | 6.493667e-01 | 8.441718e-01 | 1.314139e+00 | 1.969321e+00 | 1.542457 | 2.065858 | 1.813171 | 2.373700 | . 5 1 | -3.172026 | -3.093182 | 0.155047 | 0.343024 | 0.451619 | 0.914937 | -0.596771 | -0.827370 | -0.974472 | ... | 4.395633 | 4.958532 | 1.072512e+00 | 7.936777e-01 | 1.070915e-07 | 7.520313e-08 | 1.298665 | 0.488986 | 1.943198 | 2.112894 | . 6 1 | -3.172026 | -3.093182 | 0.188790 | 0.232964 | 0.500087 | 0.639725 | -0.083674 | 0.019814 | -4.050318 | ... | 4.390247 | 4.959537 | 8.272507e-01 | 1.036085e+00 | 6.587185e-01 | 4.546515e-01 | 1.000972 | 1.028346 | 1.824567 | 2.101414 | . 5 rows √ó 147 columns . Other ideas for feature engineering: . aggregating categorical columns by &#39;tags&#39; on features.csv | count above mean, mean abs change, abs energy | log transform, kurt transform and other transforms | get creative! | Reasons not to do more feature engineering: . We have no idea what the features represent so it might be meaningless and dangerous | The dataset is really big so adding couple more columns will make me run out of memory | Much slower computation | Split data &#9986;&#65039; . We are going to use approximately 20000 data as test set. Our target value is action which we already have defined as any weight times resp above 0.(positive trades) . # Train test split from sklearn.model_selection import train_test_split X = df.loc[:, df.columns.str.contains(&#39;feature|log&#39;)] y = np.stack([(df[c] &gt; 0).astype(&#39;int&#39;) for c in resp_cols]).T del df gc.collect() X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=42, shuffle=True) del X, y gc.collect() . Implementation #2 . Algoritms &amp; Technique . For technique, we already applied a lot of our knowledge from our EDA into our dataset. (Feature engineering, imputing nulls, dropping &lt; 85 days, etc). For algorithm, we are going to use machine learning. Now we have our data ready for training. There are hundreds of classifier model we can choose from and explore. However, after studying the Kaggle notebooks other participants have submitted, all high scored model seem to use Neural Network. I am going to try using random forest classifier and MLP to experiment here. Random Forest are always good for early because it is easy to just build and evaluate. Neural network is good at learning complicated models with the right parameter tuning. . Metrics . Since this is a multiclass-classifying problem (5 types of &#39;resp&#39; -&gt; gave us 5 pos vs neg target variables), for performance metrics we are going to use AUC(area under curve) as well as pure accuracy score for overall performance. With this metrics, we can see how our model is performing on unseen data and prevent overfitting easily to see any area for improvement accordingly. Sklearn and Seaborn provides great graphing tools for these metrics as well. . Complications . Note that the worst complication I had to face going through rest of this notebook was the size of the data. Depending on your computer&#39;s RAM size and GPU computation speed this experience will vary. In my case, I ran into out of memory a hundreds of times. To avoid this, try using cloud training. If not make sure to save your computed data frequently and clean RAM with gc.collect and del function to free up space as much as possible. . Results . Random Forest Classifier evaluation and validation . from sklearn.ensemble import RandomForestClassifier rnd_clf = RandomForestClassifier(n_estimators=100, max_leaf_nodes=32, n_jobs=-1, verbose=2) rnd_clf.fit(X_train, y_train) . test_pred = rnd_clf.predict(X_test) test_pred = np.rint(test_pred) test_acc = np.sum(test_pred == y_test)/(y_test.shape[0]*5) print(&quot;test accuracy: &quot; + str(test_acc)) . [Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers. [Parallel(n_jobs=2)]: Done 37 tasks | elapsed: 0.1s . test accuracy: 0.5242761692650334 . [Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed: 0.3s finished . resp [[3474 4349] [3205 4687]] resp_1 [[4261 3487] [3768 4199]] resp_2 [[3740 4001] [3367 4607]] resp_3 [[2823 4959] [2643 5290]] resp_4 [[3003 4858] [2743 5111]] . Result 1 justification . So we got about 52.4% accuracy with random forest. From the confusion matrix, we can tell that the model is having harder time predicting 0&#39;s correctly. It is actually doing a good job of classifying 1&#39;s though! So with this model, we can expect to get lots of good trades but also fail to not go for bad trades. . Result 1 implementation . This was our first pass solution. Although we were able to get a positive score of 52.4%, when submitted to Jane Street for Evaluation, it returned a score of 0. Meaning we have lost more profit than we gained. (The competition didn&#39;t return negative scores and only calculated positive gains). This suggests that although we were able to get more &#39;correct&#39; trades, the scale of the trades we failed to predict correctly have out-weighted our correct predictions. . MLP evaluation and validation . Classic multiple layer perceptron with AUC(Area Under Curve) metrics. After looking at many notebooks on Kaggle, MLP seem to perform the best with short run time. Let us build one ourselves. . def create_mlp( num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate ): inp = tf.keras.layers.Input(shape=(num_columns,)) x = tf.keras.layers.BatchNormalization()(inp) x = tf.keras.layers.Dropout(dropout_rates[0])(x) for i in range(len(hidden_units)): x = tf.keras.layers.Dense(hidden_units[i])(x) x = tf.keras.layers.BatchNormalization()(x) x = tf.keras.layers.Activation(tf.keras.activations.swish)(x) x = tf.keras.layers.Dropout(dropout_rates[i + 1])(x) x = tf.keras.layers.Dense(num_labels)(x) out = tf.keras.layers.Activation(&quot;sigmoid&quot;)(x) model = tf.keras.models.Model(inputs=inp, outputs=out) model.compile( optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing), metrics=tf.keras.metrics.AUC(name=&quot;AUC&quot;), ) return model . batch_size = 4096 hidden_units = [150, 150, 150] dropout_rates = [0.20, 0.20, 0.20, 0.20] label_smoothing = 1e-2 learning_rate = 3e-3 #with tpu_strategy.scope(): clf = create_mlp( X_train.shape[1], 5, hidden_units, dropout_rates, label_smoothing, learning_rate ) clf.fit(X_train, y_train, epochs=100, batch_size=batch_size) models = [] models.append(clf) . test_pred = clf.predict(X_test) test_pred = np.rint(test_pred) test_acc = np.sum(test_pred == y_test)/(y_test.shape[0]*5) print(&quot;test accuracy: &quot; + str(test_acc)) . test accuracy: 0.5501368119630926 . Resp: ROC AUC=0.544 Resp_1: ROC AUC=0.559 Resp_2: ROC AUC=0.551 Resp_3: ROC AUC=0.549 Resp_4: ROC AUC=0.547 . &lt;Figure size 2160x1440 with 0 Axes&gt; . Result 2 justification . This is actually good! Although one could say that the machine is doing slightly better than me if I was to go to Jane Street and randomly decide to &#39;action&#39; on trades. . It is important to note that even though we are getting only around ~55% accuracy only, this is actually considered good for trading markets. To explain this, since Jane Market has billions of money, as long as they have a positive return rate, it doesn&#39;t matter how much they lose because in the end they will gain more. It is like going to a casino knowing you have more chance of winning than losing. The more time you spend here, the more you will gain out of it! . Hyper-parameter tuning / Refinement . from sklearn.model_selection import RandomizedSearchCV from tensorflow.keras.wrappers.scikit_learn import KerasClassifier from tensorflow.keras.callbacks import EarlyStopping batch_size = 5000 hidden_units = [(150, 150, 150), (100,100,100), (200,200,200)] dropout_rates = [(0.25, 0.25, 0.25, 0.25), (0.3,0.3,0.3,0.3)] epochs = 100 num_columns = len(features) num_labels = 5 #num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate mlp_CV = KerasClassifier(build_fn=create_mlp, epochs=epochs, batch_size=batch_size, verbose=1) param_distributions = {&#39;hidden_units&#39;:hidden_units, &#39;learning_rate&#39;:[1e-3, 1e-4], &#39;label_smoothing&#39;:[1e-2, 1e-1], &#39;dropout_rates&#39;:dropout_rates, &#39;num_columns&#39;: [len(features)], &#39;num_labels&#39;: [5]} random_cv = RandomizedSearchCV(estimator=mlp_CV, param_distributions=param_distributions, n_iter=5, n_jobs=-1, cv=3, random_state=42) random_cv.fit(X_train, y_train, callbacks=[EarlyStopping(patience=10)])#, epochs=200, batch_size=5000) models = [] models.append(random_cv) . RandomSearch and GridSearch easily runs out of memory.. . So from trial and error, I&#39;ve learned that with learning rate at 1e-3, model overfits quickly around at 10 with batch_size around 5000. However, the model wasn&#39;t able to learn much with less than 100 epochs. One solution is to add more layers and perceptrons which is what I did and the result 2 is the result of manual hyper param tuning. Before the model was definetly at around 200 epochs with same learning rate with 5000 batches giving me an accuracy of only 51%. After manual hyperparameter, (running few different param combination by myself) I was able to increase about 3.5% accuracy! . Conclusion . For my final review and conclusion, check out my blog post . Other things to try/explore: . Weighted training. We know that sometimes we will encounter &#39;monster&#39; deals. It is crucial for the Kaggle competition to get these ones correct since these will probably outweight most other trades. So we could make model that focuses more on these heavy trades. (high weight X resp data) | Split data and train multiple models. Idea is that we could split the data into two by feature_0 and maybe one model that optimizes the &#39;1&#39;s data and another model that optimizes the &#39;-1&#39;s data. | Make much more features and explore more data (requires time and big data machines) | One interesting thing I learned is that apparently, in financial, it is sometimes good to heavily overfit the model. Something to do with volatile. I&#39;ve experimented with this and indeed my utility score for the competition went really high when super overfitted with epoches over 200. | Submission . th = 0.5 f = np.median models = models[-3:] . def feature_engineering(df): tag_0_group = [9, 10, 19, 20, 29, 30, 56, 73, 79, 85, 91, 97, 103, 109, 115, 122, 123] for col in tag_0_group: df[&#39;log_&#39;+str(col)] = (df[&#39;feature_&#39;+str(col)]-df[&#39;feature_&#39;+str(col)].min()+1).transform(np.log) return df . import janestreet #env = janestreet.make_env() for (test_df, pred_df) in tqdm(env.iter_test()): if test_df[&#39;weight&#39;].item() &gt; 0: x_tt = test_df.loc[:, features] x_tt = feature_engineering(x_tt).values if np.isnan(x_tt[:, 1:].sum()): x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean pred = np.mean([model(x_tt, training = False).numpy() for model in models],axis=0) pred = f(pred) pred_df.action = np.where(pred &gt;= th, 1, 0).astype(int) else: pred_df.action = 0 env.predict(pred_df) . Reference . Imputing-missing-values OWN Jane Street with Keras NN .",
            "url": "https://leejaeka.github.io/jaekangai/mlp/python/feature%20engineering/imputation/jane%20street/kaggle/visualization/big%20data/random%20forest/2021/01/25/jane-predict.html",
            "relUrl": "/mlp/python/feature%20engineering/imputation/jane%20street/kaggle/visualization/big%20data/random%20forest/2021/01/25/jane-predict.html",
            "date": " ‚Ä¢ Jan 25, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Jane Street Market EDA üìà",
            "content": "Project Definition &#127942; . The project is based on Kaggle competition by Jane Street - Jane Street Market Prediction &quot;Buy low, sell high&quot; sounds easy. In reality, we know trading is difficult to solve and even more so in today&#39;s fast financial markets. Developing strategy with machine learning model can help us maximize returns using market data from a major global stock exchange. Then the competition will take our predictiveness to model against future market returns and give feedback on the leaderboard. My goal is to explore financial area of data science and explore Kaggle community as much as possible. . In general, if one is able to generate a highly predictive model which selects the right trades to execute, they would also be playing an important role in sending the market signals that push prices closer to ‚Äúfair‚Äù values. That is, a better model will mean the market will be more efficient going forward. However, developing good models will be challenging for many reasons, including a very low signal-to-noise ratio, potential redundancy, strong feature correlation, and difficulty of coming up with a proper mathematical formulation. (src: https://www.kaggle.com/c/jane-street-market-prediction/overview/description) . Dataset: 6GB of Real world financial markets. . anonymized set of features, feature_{0...129}, representing real stock market data. | each row in the dataset represents a trading opportunity, for which you will be predicting an action value: 1 to make the trade and 0 to pass on it. | each trade has an associated weight and resp, which together represents a return on the trade. | date column is an integer which represents the day of the trade, while ts_id represents a time ordering. | in addition to anonymized feature values, you are provided with metadata about the features in features.csv. | more info: https://www.kaggle.com/c/jane-street-market-prediction/data | . Problem: Predict &#39;Action&#39; columns . Where 1 to make the trade and 0 to pass on it. The goal is minimize choosing bad trades and maximizing good trades. . Proposing a solution . Even the world&#39;s finest financial expert wouldn&#39;t be able to gain anything from this anonymized data. Hence we are forced to use machine learning or data science approach to solve this problem. Since the goal is clear (maximize profit) we want to see which features seem important or strange through EDA and visualizations. We will also have to grasp understanding of the data so it is not just a big 6GB of numbers. This will help us decide what to do with missing values and do feature engineering to help the computer learn. Then we are going to build classifier model to let the computer do the hard work of learning to make good predictions. Of course, these models will be terrible at first so we will have to supervise it closely with some hyperparameters and strict evaluations. Finally, the Jane Street will evaluate our submission and tell us if it is any good : ) . Metrics . The competition is evaluated on a utility score. Each row in the test set represents a trading opportunity for which you will be predicting an action value. Each trade j has an associated weight and resp, which represents a return. Weight and resp are not defined on test data set. . For each date i, we define: . $pi=‚àë_{j}(weight_{ij}‚àóresp_{ij}‚àóaction{i_j})$ . $t= dfrac{‚àëp_i}{‚àëp_{i}^2} sqrt{ dfrac{250}{|i|}}$ . where |i| is the number of unique dates in the test set. The utility is then defined as: $u=min(max(t,0),6)‚àëpi.$ . EDA and Visualization . Import Data &#128218; . df = dt.fread(&#39;../../../Kaggle/Jane-Street-Market-Prediction/input/train.csv&#39;) df = df.to_pandas() . feat = pd.read_csv(&quot;../../../Kaggle/Jane-Street-Market-Prediction/input/features.csv&quot;) . df.shape: (2390491, 138) how many days? 500days . date weight resp_1 resp_2 resp_3 resp_4 resp feature_0 feature_1 feature_2 ... feature_121 feature_122 feature_123 feature_124 feature_125 feature_126 feature_127 feature_128 feature_129 ts_id . 0 0 | 0.000000 | 0.009916 | 0.014079 | 0.008773 | 0.001390 | 0.006270 | 1 | -1.872746 | -2.191242 | ... | NaN | 1.168391 | 8.313583 | 1.782433 | 14.018213 | 2.653056 | 12.600292 | 2.301488 | 11.445807 | 0 | . 1 0 | 16.673515 | -0.002828 | -0.003226 | -0.007319 | -0.011114 | -0.009792 | -1 | -1.349537 | -1.704709 | ... | NaN | -1.178850 | 1.777472 | -0.915458 | 2.831612 | -1.417010 | 2.297459 | -1.304614 | 1.898684 | 1 | . 2 0 | 0.000000 | 0.025134 | 0.027607 | 0.033406 | 0.034380 | 0.023970 | -1 | 0.812780 | -0.256156 | ... | NaN | 6.115747 | 9.667908 | 5.542871 | 11.671595 | 7.281757 | 10.060014 | 6.638248 | 9.427299 | 2 | . 3 0 | 0.000000 | -0.004730 | -0.003273 | -0.000461 | -0.000476 | -0.003200 | -1 | 1.174378 | 0.344640 | ... | NaN | 2.838853 | 0.499251 | 3.033732 | 1.513488 | 4.397532 | 1.266037 | 3.856384 | 1.013469 | 3 | . 4 0 | 0.138531 | 0.001252 | 0.002165 | -0.001215 | -0.006219 | -0.002604 | 1 | -3.172026 | -3.093182 | ... | NaN | 0.344850 | 4.101145 | 0.614252 | 6.623456 | 0.800129 | 5.233243 | 0.362636 | 3.926633 | 4 | . 5 rows √ó 138 columns . df.describe() . date weight resp_1 resp_2 resp_3 resp_4 resp feature_0 feature_1 feature_2 ... feature_121 feature_122 feature_123 feature_124 feature_125 feature_126 feature_127 feature_128 feature_129 ts_id . count 2.390491e+06 | 2.390491e+06 | 2.390491e+06 | 2.390491e+06 | 2.390491e+06 | 2.390491e+06 | 2.390491e+06 | 2.390491e+06 | 2.390491e+06 | 2.390491e+06 | ... | 2.320637e+06 | 2.390268e+06 | 2.390268e+06 | 2.374408e+06 | 2.374408e+06 | 2.381638e+06 | 2.381638e+06 | 2.388570e+06 | 2.388570e+06 | 2.390491e+06 | . mean 2.478668e+02 | 3.031535e+00 | 1.434969e-04 | 1.980749e-04 | 2.824183e-04 | 4.350201e-04 | 4.083113e-04 | 9.838565e-03 | 3.855776e-01 | 3.576875e-01 | ... | 2.687757e-01 | 3.435523e-01 | 2.799973e-01 | 3.351537e-01 | 2.448752e-01 | 3.391778e-01 | 2.323809e-01 | 3.425608e-01 | 2.456182e-01 | 1.195245e+06 | . std 1.522746e+02 | 7.672794e+00 | 8.930163e-03 | 1.230236e-02 | 1.906882e-02 | 3.291224e-02 | 2.693609e-02 | 9.999518e-01 | 2.559373e+00 | 2.477335e+00 | ... | 2.174238e+00 | 2.087842e+00 | 1.977643e+00 | 1.742587e+00 | 2.242853e+00 | 2.534498e+00 | 1.795854e+00 | 2.307130e+00 | 1.765419e+00 | 6.900755e+05 | . min 0.000000e+00 | 0.000000e+00 | -3.675043e-01 | -5.328334e-01 | -5.681196e-01 | -5.987447e-01 | -5.493845e-01 | -1.000000e+00 | -3.172026e+00 | -3.093182e+00 | ... | -7.471971e+00 | -5.862979e+00 | -6.029281e+00 | -4.080720e+00 | -8.136407e+00 | -8.215050e+00 | -5.765982e+00 | -7.024909e+00 | -5.282181e+00 | 0.000000e+00 | . 25% 1.040000e+02 | 1.617400e-01 | -1.859162e-03 | -2.655044e-03 | -5.030704e-03 | -9.310415e-03 | -7.157903e-03 | -1.000000e+00 | -1.299334e+00 | -1.263628e+00 | ... | -1.123252e+00 | -1.114326e+00 | -9.512009e-01 | -9.133750e-01 | -1.212124e+00 | -1.452912e+00 | -8.993050e-01 | -1.278341e+00 | -8.544535e-01 | 5.976225e+05 | . 50% 2.540000e+02 | 7.086770e-01 | 4.552665e-05 | 6.928179e-05 | 1.164734e-04 | 1.222579e-04 | 8.634997e-05 | 1.000000e+00 | -1.870182e-05 | -7.200577e-07 | ... | 0.000000e+00 | 7.006244e-17 | 6.054629e-17 | 4.870826e-17 | -2.558675e-16 | 1.015055e-16 | 5.419920e-17 | 8.563069e-17 | 4.869529e-17 | 1.195245e+06 | . 75% 3.820000e+02 | 2.471791e+00 | 2.097469e-03 | 2.939111e-03 | 5.466336e-03 | 9.804649e-03 | 7.544347e-03 | 1.000000e+00 | 1.578417e+00 | 1.526399e+00 | ... | 1.342829e+00 | 1.405926e+00 | 1.308625e+00 | 1.228277e+00 | 1.409687e+00 | 1.767275e+00 | 1.111491e+00 | 1.582633e+00 | 1.125321e+00 | 1.792868e+06 | . max 4.990000e+02 | 1.672937e+02 | 2.453477e-01 | 2.949339e-01 | 3.265597e-01 | 5.113795e-01 | 4.484616e-01 | 1.000000e+00 | 7.442989e+01 | 1.480763e+02 | ... | 1.107771e+02 | 4.812516e+01 | 1.276908e+02 | 6.514517e+01 | 7.052807e+01 | 5.872849e+01 | 6.932221e+01 | 5.119038e+01 | 1.164568e+02 | 2.390490e+06 | . 8 rows √ó 138 columns . feat.describe() . feature tag_0 tag_1 tag_2 tag_3 tag_4 tag_5 tag_6 tag_7 tag_8 ... tag_19 tag_20 tag_21 tag_22 tag_23 tag_24 tag_25 tag_26 tag_27 tag_28 . count 130 | 130 | 130 | 130 | 130 | 130 | 130 | 130 | 130 | 130 | ... | 130 | 130 | 130 | 130 | 130 | 130 | 130 | 130 | 130 | 130 | . unique 130 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | ... | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | . top feature_25 | False | False | False | False | False | False | False | False | False | ... | False | False | False | False | False | False | False | False | False | False | . freq 1 | 113 | 113 | 113 | 113 | 113 | 122 | 90 | 128 | 128 | ... | 123 | 125 | 125 | 121 | 82 | 118 | 118 | 118 | 118 | 118 | . 4 rows √ó 30 columns . As told, all the featues and even tags are anonymized. There&#39;s not much human interpretability just from describe tables. Except feature_0 is unique by being binary. . Cleaning Data &#129529; . There are 88 many cols with at least one null value {&#39;feature_108&#39;, &#39;feature_91&#39;, &#39;feature_115&#39;, &#39;feature_128&#39;, &#39;feature_93&#39;, &#39;feature_33&#39;, &#39;feature_24&#39;, &#39;feature_4&#39;, &#39;feature_79&#39;, &#39;feature_28&#39;, &#39;feature_19&#39;, &#39;feature_88&#39;, &#39;feature_56&#39;, &#39;feature_117&#39;, &#39;feature_31&#39;, &#39;feature_21&#39;, &#39;feature_7&#39;, &#39;feature_94&#39;, &#39;feature_16&#39;, &#39;feature_76&#39;, &#39;feature_96&#39;, &#39;feature_12&#39;, &#39;feature_55&#39;, &#39;feature_29&#39;, &#39;feature_120&#39;, &#39;feature_35&#39;, &#39;feature_124&#39;, &#39;feature_32&#39;, &#39;feature_74&#39;, &#39;feature_17&#39;, &#39;feature_116&#39;, &#39;feature_97&#39;, &#39;feature_86&#39;, &#39;feature_105&#39;, &#39;feature_127&#39;, &#39;feature_36&#39;, &#39;feature_99&#39;, &#39;feature_34&#39;, &#39;feature_104&#39;, &#39;feature_10&#39;, &#39;feature_100&#39;, &#39;feature_58&#39;, &#39;feature_87&#39;, &#39;feature_111&#39;, &#39;feature_122&#39;, &#39;feature_80&#39;, &#39;feature_78&#39;, &#39;feature_25&#39;, &#39;feature_18&#39;, &#39;feature_59&#39;, &#39;feature_26&#39;, &#39;feature_73&#39;, &#39;feature_92&#39;, &#39;feature_15&#39;, &#39;feature_81&#39;, &#39;feature_27&#39;, &#39;feature_13&#39;, &#39;feature_112&#39;, &#39;feature_109&#39;, &#39;feature_125&#39;, &#39;feature_3&#39;, &#39;feature_98&#39;, &#39;feature_82&#39;, &#39;feature_84&#39;, &#39;feature_45&#39;, &#39;feature_90&#39;, &#39;feature_9&#39;, &#39;feature_8&#39;, &#39;feature_118&#39;, &#39;feature_75&#39;, &#39;feature_123&#39;, &#39;feature_22&#39;, &#39;feature_11&#39;, &#39;feature_23&#39;, &#39;feature_44&#39;, &#39;feature_20&#39;, &#39;feature_114&#39;, &#39;feature_106&#39;, &#39;feature_14&#39;, &#39;feature_102&#39;, &#39;feature_129&#39;, &#39;feature_110&#39;, &#39;feature_85&#39;, &#39;feature_126&#39;, &#39;feature_121&#39;, &#39;feature_30&#39;, &#39;feature_103&#39;, &#39;feature_72&#39;} . A lot of the histogram of above features has extreme outliers. For the full enlarged version of the histograms, check out here It would be safe to fill the null values with medians. Other imputation method considered were mean and KNN-Imputation. Check out my other notebook where KNN-Imputation was used to train MLP. . feature with most nans: feature_27, with 395535 . If we just remove all nans, we would be removing more than 16.54% of the dataset. . Interesting points so far: . feature_0 is binary. | A lot of features seems to be normally distributed. | A lot of missing values. | . Plots &amp; Visualization &#128202; . resp, resp_1, resp_2, resp_3, resp_4 . We can see that resp is closely related to resp_4 (blue and purple). Resp_1 and resp_2 also seem to be closely related but much much linear. Resp_3 seem to be in the middle, where the shape is closer to upper group but position is slightly closer to green and orange. . Weights . Note: weight and resp multiplied together represents a return on the trade. . We can see that most weights are around 0.2 and we can see two &#39;peaks&#39; which is around 0.2 and 0.3. Note that maximum weight was 167.29 represented by 1.0 on x-axis. Thus 0.2 represents around 33.458 and 0.3 represents around 50.187. . 2921 . Note that the graph plots all the positive gains. (Our 1&#39;s for our action column). So we can see that there were &#39;bigger&#39; gains in the beginning and as time approach 500, the gain becomes smaller. In conclusion, the earlier trades are much bigger but we don&#39;t know what it&#39;s going to be like in our competition test set. . We know that we probability want to invest more &#39;weight&#39; if there are bigger &#39;resp&#39;(return). We learn here that higher weights are only when resp is close to 0. In other words, it is dumb to trade if resp is away from 0 but it is safe to invest even a lot if it is near 0. . 74963 . In the Kaggle community, there&#39;s been lots of discussion on how the trends changed significantly since day ~85. We can see much more trades happening before day 100. Rest of the days are still very active but not as noisy. We can suggest that there has been a change of trading model from Jane Street as discussed here by Carl. . Let us look at the most important feature, &#39;feature_0&#39; . df[&#39;feature_0&#39;].value_counts() . 1 1207005 -1 1183486 Name: feature_0, dtype: int64 . Interestingly, when feature_0 is 1, plot shows negative slope while in contrast, when feature_0 is -1, plot shows positive slope. My guess is that feature_0 corresponds to Buy(1) and Sell(-1) or vice versa. So if we set action to 1 with feature_0 = 1 then we are selling and when we set action to 0 with feature_0 = -1, then we are buying. This makes sense since whether we are buying or selling we can still lose or gain profit. . Features . Remember that we have another file called features.csv. Which can help us understand 100+ features and maybe cluster into groups. Let&#39;s take a look. . Let us see what tag_0 groups tells us. . &lt;AxesSubplot:&gt; . Correlation between features of tag_0. It looks like there certainly are correlation between elements of the group except a few. . Interesting points: . feature_0 has no tags | feature 79 to 119 all has 4 tags | feature 7 to 36 have 3 and 4 tags periodically | Similar trend between 2 to 7, 37 to 40, 120 to 129 | tag_n doesn&#39;t tell too much about the features | . Reference &#128214; . Jane Street: EDA of day 0 and feature importance | Jane_street_Extensive_EDA &amp; PCA starter üìä‚ö° | EDA / A Quant&#39;s Prespective | . Submission . In another notebook. . Implementation Planning . Thoughts going into predicting phase. . Days before ~100 can be dropped as suspicion of model shift. | Feature_0 seem very important to find slope of cummulative resp. | Resp near 0 is prefered over other values. | A lot of features are normally distributed. | We have over 2 million datas, it would be safe to add lot more features(feature enginerring) | There are a lot of missing values too. Can try mean, median or KNN imputation methods. | Note that although this is kind of a time series data, we can only predict with features 0 to 129 |",
            "url": "https://leejaeka.github.io/jaekangai/python/eda/jane%20street/kaggle/visualization/big%20data/2021/01/23/JaneStreet-Copy1.html",
            "relUrl": "/python/eda/jane%20street/kaggle/visualization/big%20data/2021/01/23/JaneStreet-Copy1.html",
            "date": " ‚Ä¢ Jan 23, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Recommendations with IBM üò∫",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt import project_tests as t import pickle %matplotlib inline df = pd.read_csv(&#39;data/user-item-interactions.csv&#39;) df_content = pd.read_csv(&#39;data/articles_community.csv&#39;) del df[&#39;Unnamed: 0&#39;] del df_content[&#39;Unnamed: 0&#39;] # Show df to get an idea of the data df.head() . article_id title email . 0 1430.0 | using pixiedust for fast, flexible, and easier... | ef5f11f77ba020cd36e1105a00ab868bbdbf7fe7 | . 1 1314.0 | healthcare python streaming application demo | 083cbdfa93c8444beaa4c5f5e0f5f9198e4f9e0b | . 2 1429.0 | use deep learning for image classification | b96a4f2e92d8572034b1e9b28f9ac673765cd074 | . 3 1338.0 | ml optimization using cognitive assistant | 06485706b34a5c9bf2a0ecdac41daf7e7654ceb7 | . 4 1276.0 | deploy your python model as a restful api | f01220c46fc92c6e6b161b1849de11faacd7ccb2 | . df_content.head() . doc_body doc_description doc_full_name doc_status article_id . 0 Skip navigation Sign in SearchLoading... r n r... | Detect bad readings in real time using Python ... | Detect Malfunctioning IoT Sensors with Streami... | Live | 0 | . 1 No Free Hunch Navigation * kaggle.com r n r n ... | See the forest, see the trees. Here lies the c... | Communicating data science: A guide to present... | Live | 1 | . 2 ‚ò∞ * Login r n * Sign Up r n r n * Learning Pat... | Here‚Äôs this week‚Äôs news in Data Science and Bi... | This Week in Data Science (April 18, 2017) | Live | 2 | . 3 DATALAYER: HIGH THROUGHPUT, LOW LATENCY AT SCA... | Learn how distributed DBs solve the problem of... | DataLayer Conference: Boost the performance of... | Live | 3 | . 4 Skip navigation Sign in SearchLoading... r n r... | This video demonstrates the power of IBM DataS... | Analyze NY Restaurant data using Spark in DSX | Live | 4 | . Part I : Exploratory Data Analysis . df[&#39;email&#39;].value_counts().median() . 3.0 . df[&#39;email&#39;].value_counts().max() . 364 . print(df[&#39;email&#39;].describe()) df[&#39;email&#39;].value_counts().hist(bins=500) plt.xscale(&#39;log&#39;) plt.show() . count 45976 unique 5148 top 2b6c0f514c2f2b04ad3c4583407dccd0810469ee freq 364 Name: email, dtype: object . Explore and remove duplicate articles from the df_content dataframe. . df_content[df_content.duplicated(subset=[&#39;article_id&#39;])] . doc_body doc_description doc_full_name doc_status article_id . 365 Follow Sign in / Sign up Home About Insight Da... | During the seven-week Insight Data Engineering... | Graph-based machine learning | Live | 50 | . 692 Homepage Follow Sign in / Sign up Homepage * H... | One of the earliest documented catalogs was co... | How smart catalogs can turn the big data flood... | Live | 221 | . 761 Homepage Follow Sign in Get started Homepage *... | Today‚Äôs world of data science leverages data f... | Using Apache Spark as a parallel processing fr... | Live | 398 | . 970 This video shows you how to construct queries ... | This video shows you how to construct queries ... | Use the Primary Index | Live | 577 | . 971 Homepage Follow Sign in Get started * Home r n... | If you are like most data scientists, you are ... | Self-service data preparation with IBM Data Re... | Live | 232 | . df_content = df_content.drop_duplicates(subset=[&#39;article_id&#39;]) . len(df_content[df_content.duplicated(subset=[&#39;article_id&#39;])]) # check . 0 . print(len(df[&#39;article_id&#39;].unique())) print(len(df_content[&#39;article_id&#39;].unique())) print(len(df[&#39;email&#39;].unique())) len(df) . 714 1051 5149 . 45993 . find the most viewed article_id, as well as how often it was viewed. . df[&#39;article_id&#39;].value_counts().head() . 1429.0 937 1330.0 927 1431.0 671 1427.0 643 1364.0 627 Name: article_id, dtype: int64 . def email_mapper(): coded_dict = dict() cter = 1 email_encoded = [] for val in df[&#39;email&#39;]: if val not in coded_dict: coded_dict[val] = cter cter+=1 email_encoded.append(coded_dict[val]) return email_encoded email_encoded = email_mapper() del df[&#39;email&#39;] df[&#39;user_id&#39;] = email_encoded # show header df.head() . article_id title user_id . 0 1430.0 | using pixiedust for fast, flexible, and easier... | 1 | . 1 1314.0 | healthcare python streaming application demo | 2 | . 2 1429.0 | use deep learning for image classification | 3 | . 3 1338.0 | ml optimization using cognitive assistant | 4 | . 4 1276.0 | deploy your python model as a restful api | 5 | . Part II: Rank-Based Recommendations . Unlike in the earlier lessons, we don&#39;t actually have ratings for whether a user liked an article or not. We only know that a user has interacted with an article. In these cases, the popularity of an article can really only be based on how often an article was interacted with. . return the n top articles ordered with most interactions as the top. . def get_top_articles(n, df=df): &#39;&#39;&#39; INPUT: n - (int) the number of top articles to return df - (pandas dataframe) df as defined at the top of the notebook OUTPUT: top_articles - (list) A list of the top &#39;n&#39; article titles &#39;&#39;&#39; # Your code here result = [] top_articles_idx = df[&#39;article_id&#39;].value_counts().index[:n] for id in top_articles_idx: result.append(list(df[df[&#39;article_id&#39;]==id][&#39;title&#39;].drop_duplicates())[0]) return result # Return the top article titles from df (not df_content) def get_top_article_ids(n, df=df): &#39;&#39;&#39; INPUT: n - (int) the number of top articles to return df - (pandas dataframe) df as defined at the top of the notebook OUTPUT: top_articles - (list) A list of the top &#39;n&#39; article titles &#39;&#39;&#39; # Your code here return list(df[&#39;article_id&#39;].value_counts().index[:n]) # Return the top article ids . print(get_top_articles(10)) print(get_top_article_ids(10)) . [&#39;use deep learning for image classification&#39;, &#39;insights from new york car accident reports&#39;, &#39;visualize car data with brunel&#39;, &#39;use xgboost, scikit-learn &amp; ibm watson machine learning apis&#39;, &#39;predicting churn with the spss random tree algorithm&#39;, &#39;healthcare python streaming application demo&#39;, &#39;finding optimal locations of new store using decision optimization&#39;, &#39;apache spark lab, part 1: basic concepts&#39;, &#39;analyze energy consumption in buildings&#39;, &#39;gosales transactions for logistic regression model&#39;] [1429.0, 1330.0, 1431.0, 1427.0, 1364.0, 1314.0, 1293.0, 1170.0, 1162.0, 1304.0] . Part III: User-User Based Collaborative Filtering . reformat the df dataframe to be shaped with users as the rows and articles as the columns. . If a user has interacted with an article, then place a 1 where the user-row meets for that article-column. It does not matter how many times a user has interacted with the article, all entries where a user has interacted with an article should be a 1. | . If a user has not interacted with an item, then place a zero where the user-row meets for that article-column. | . def create_user_item_matrix(df): &#39;&#39;&#39; INPUT: df - pandas dataframe with article_id, title, user_id columns OUTPUT: user_item - user item matrix Description: Return a matrix with user ids as rows and article ids on the columns with 1 values where a user interacted with an article and a 0 otherwise &#39;&#39;&#39; # Fill in the function here #user_item = pd.pivot_table(df, index=[&#39;user_id&#39;], columns=[&#39;article_id&#39;], values=[&#39;title&#39;],aggfunc=[np.sum], fill_value=0) user_item = df.groupby([&#39;user_id&#39;, &#39;article_id&#39;])[&#39;title&#39;].count().unstack().notnull().astype(int).fillna(0) return user_item # return the user_item matrix user_item = create_user_item_matrix(df) . take a user_id and provide an ordered list of the most similar users to that user (from most similar to least similar). The returned result should not contain the provided user_id, as we know that each user is similar to him/herself. Because the results for each user here are binary, it (perhaps) makes sense to compute similarity as the dot product of two users. . def find_similar_users(user_id, user_item=user_item): &#39;&#39;&#39; INPUT: user_id - (int) a user_id user_item - (pandas dataframe) matrix of users by articles: 1&#39;s when a user has interacted with an article, 0 otherwise OUTPUT: similar_users - (list) an ordered list where the closest users (largest dot product users) are listed first Description: Computes the similarity of every pair of users based on the dot product Returns an ordered &#39;&#39;&#39; # compute similarity of each user to the provided user # sort by similarity similarities = user_item.dot(user_item.iloc[user_id-1]).sort_values(ascending=False) # create list of just the ids ids = similarities.index # remove the own user&#39;s id most_similar_users = ids return list(most_similar_users.drop(user_id)) # return a list of the users in order from most to least similar . return the articles you would recommend to each user. . def get_article_names(article_ids, df=df): &#39;&#39;&#39; INPUT: article_ids - (list) a list of article ids df - (pandas dataframe) df as defined at the top of the notebook OUTPUT: article_names - (list) a list of article names associated with the list of article ids (this is identified by the title column) &#39;&#39;&#39; article_names = [] for i in article_ids: article_names += list(df[df[&#39;article_id&#39;].astype(str) == i][&#39;title&#39;].unique()) return article_names # Return the article names associated with list of article ids def get_user_articles(user_id, user_item=user_item): &#39;&#39;&#39; INPUT: user_id - (int) a user id user_item - (pandas dataframe) matrix of users by articles: 1&#39;s when a user has interacted with an article, 0 otherwise OUTPUT: article_ids - (list) a list of the article ids seen by the user article_names - (list) a list of article names associated with the list of article ids (this is identified by the doc_full_name column in df_content) Description: Provides a list of the article_ids and article titles that have been seen by a user &#39;&#39;&#39; # Your code here article_ids = user_item.iloc[user_id-1] article_ids = [i for i in article_ids[article_ids == 1].index.astype(str)] article_names = get_article_names(article_ids) return article_ids, article_names # return the ids and names def user_user_recs(user_id, m=10): &#39;&#39;&#39; INPUT: user_id - (int) a user id m - (int) the number of recommendations you want for the user OUTPUT: recs - (list) a list of recommendations for the user Description: Loops through the users based on closeness to the input user_id For each user - finds articles the user hasn&#39;t seen before and provides them as recs Does this until m recommendations are found Notes: Users who are the same closeness are chosen arbitrarily as the &#39;next&#39; user For the user where the number of recommended articles starts below m and ends exceeding m, the last items are chosen arbitrarily &#39;&#39;&#39; # Your code here recs = set() i = 0 user_articles, _ = get_user_articles(user_id) while len(recs) &lt; m: similar_user_ids = find_similar_users(user_id) similar_user_articles, _ = get_user_articles(similar_user_ids[i]) recs.update(list(set(similar_user_articles)-set(user_articles))) i += 1 return list(recs)[:m] # return your recommendations for this user_id . get_article_names(user_user_recs(1, 10)) # Return 10 recommendations for user 1 . [&#39;502 forgetting the past to learn the future: long ... nName: title, dtype: object&#39;, &#39;discover hidden facebook usage insights&#39;, &#39;aspiring data scientists! start to learn statistics with these 6 books!&#39;, &#39;using bigdl in dsx for deep learning on spark&#39;, &#39;using machine learning to predict parking difficulty&#39;, &#39;data science platforms are on the rise and ibm is leading the way&#39;, &#39;a dynamic duo ‚Äì inside machine learning ‚Äì medium&#39;, &#39;this week in data science (february 14, 2017)&#39;, &#39;what is smote in an imbalanced class setting (e.g. fraud detection)?&#39;, &#39;shaping data with ibm data refinery&#39;] . improve the consistency of the user_user_recs function from above. . Instead of arbitrarily choosing when we obtain users who are all the same closeness to a given user - choose the users that have the most total article interactions before choosing those with fewer article interactions. | . Instead of arbitrarily choosing articles from the user where the number of recommended articles starts below m and ends exceeding m, choose articles with the articles with the most total interactions before choosing those with fewer total interactions. This ranking should be what would be obtained from the top_articles function you wrote earlier. | . def get_top_sorted_users(user_id, df=df, user_item=user_item): &#39;&#39;&#39; INPUT: user_id - (int) df - (pandas dataframe) df as defined at the top of the notebook user_item - (pandas dataframe) matrix of users by articles: 1&#39;s when a user has interacted with an article, 0 otherwise OUTPUT: neighbors_df - (pandas dataframe) a dataframe with: neighbor_id - is a neighbor user_id similarity - measure of the similarity of each user to the provided user_id num_interactions - the number of articles viewed by the user - if a u Other Details - sort the neighbors_df by the similarity and then by number of interactions where highest of each is higher in the dataframe &#39;&#39;&#39; # Your code here similar_user = pd.DataFrame(find_similar_users(user_id)) num_interactions = df[&#39;user_id&#39;].value_counts() similarities = user_item.dot(user_item.iloc[user_id-1]).sort_values(ascending=False) neighbors_df = num_interactions.to_frame(name=&#39;num_interactions&#39;).merge(pd.DataFrame(similarities).drop(user_id).reset_index(), right_on=&quot;user_id&quot;, left_index=True) neighbors_df = neighbors_df.sort_values(by=[0, &#39;num_interactions&#39;], ascending=False) neighbors_df = neighbors_df.rename(columns={0:&quot;similarity&quot;,&quot;user_id&quot;:&quot;neighbor_id&quot;}) return neighbors_df # Return the dataframe specified in the doc_string . get_top_sorted_users(20) . num_interactions neighbor_id similarity . 13 116 | 170 | 2 | . 12 114 | 3169 | 2 | . 9 97 | 204 | 2 | . 15 95 | 5138 | 2 | . 0 78 | 40 | 2 | . ... ... | ... | ... | . 2110 1 | 1039 | 0 | . 4091 1 | 3150 | 0 | . 2041 1 | 1103 | 0 | . 4947 1 | 3182 | 0 | . 1356 1 | 2049 | 0 | . 5148 rows √ó 3 columns . def user_user_recs_part2(user_id, m=10): &#39;&#39;&#39; INPUT: user_id - (int) a user id m - (int) the number of recommendations you want for the user OUTPUT: recs - (list) a list of recommendations for the user by article id rec_names - (list) a list of recommendations for the user by article title Description: Loops through the users based on closeness to the input user_id For each user - finds articles the user hasn&#39;t seen before and provides them as recs Does this until m recommendations are found Notes: * Choose the users that have the most total article interactions before choosing those with fewer article interactions. * Choose articles with the articles with the most total interactions before choosing those with fewer total interactions. &#39;&#39;&#39; # Your code here recs = [] top_df = get_top_sorted_users(user_id) user_article_ids, _ = get_user_articles(user_id) i = 0 while len(recs) &lt; m: similar_user_articles, _ = get_user_articles(top_df[&#39;neighbor_id&#39;][i]) possible_recommendations = (list(set([str(j) for j in similar_user_articles])-set(user_article_ids))) top_articles = list(df[df[&#39;article_id&#39;].isin(possible_recommendations)][&#39;article_id&#39;].value_counts(ascending=False).index.astype(str)) recs += top_articles i += 1 recs = recs[:m] rec_names = get_article_names(recs) return recs, rec_names . rec_ids, rec_names = user_user_recs_part2(20, 10) print(&quot;The top 10 recommendations for user 20 are the following article ids:&quot;) print(rec_ids) print() print(&quot;The top 10 recommendations for user 20 are the following article names:&quot;) print(rec_names) . The top 10 recommendations for user 20 are the following article ids: [&#39;1429.0&#39;, &#39;1330.0&#39;, &#39;1431.0&#39;, &#39;1427.0&#39;, &#39;1364.0&#39;, &#39;1314.0&#39;, &#39;1162.0&#39;, &#39;1304.0&#39;, &#39;43.0&#39;, &#39;1351.0&#39;] The top 10 recommendations for user 20 are the following article names: [&#39;use deep learning for image classification&#39;, &#39;insights from new york car accident reports&#39;, &#39;visualize car data with brunel&#39;, &#39;use xgboost, scikit-learn &amp; ibm watson machine learning apis&#39;, &#39;predicting churn with the spss random tree algorithm&#39;, &#39;healthcare python streaming application demo&#39;, &#39;analyze energy consumption in buildings&#39;, &#39;gosales transactions for logistic regression model&#39;, &#39;deep learning with tensorflow course by big data university&#39;, &#39;model bike sharing data with spss&#39;] . user1_most_sim = (get_top_sorted_users(1)[&#39;neighbor_id&#39;][0])# Find the user that is most similar to user 1 user131_10th_sim = (get_top_sorted_users(131)[&#39;neighbor_id&#39;][9])# Find the 10th most similar user to user 131 . We can use get_top_article_ids(n) function to recommend the top articles with most interactions. This will work in general cases since we can assume these articles were more interacted because it drew more people&#39;s interest. Another better way to make recommendation to new user may be to recommend top articles but in order of newest. The idea is that it is still popular and it reduces risk of recommending articles based on date it is created. (Since the longer it exists, the more chance it has more interactions than newer ones without much special attraction) . Part IV: Matrix Factorization . build use matrix factorization to make article recommendations to the users on the IBM Watson Studio platform. . user_item_matrix = pd.read_pickle(&#39;user_item_matrix.p&#39;) . user_item_matrix.head() . article_id 0.0 100.0 1000.0 1004.0 1006.0 1008.0 101.0 1014.0 1015.0 1016.0 ... 977.0 98.0 981.0 984.0 985.0 986.0 990.0 993.0 996.0 997.0 . user_id . 1 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 3 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 5 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 5 rows √ó 714 columns . u, s, vt = np.linalg.svd(user_item_matrix)# use the built in to get the three matrices . Remember that SVD require there are no NAN values!! . to get an idea of how the accuracy improves as we increase the number of latent features. . num_latent_feats = np.arange(10,700+10,20) sum_errs = [] for k in num_latent_feats: # restructure with k latent features s_new, u_new, vt_new = np.diag(s[:k]), u[:, :k], vt[:k, :] # take dot product user_item_est = np.around(np.dot(np.dot(u_new, s_new), vt_new)) # compute error for each prediction to actual value diffs = np.subtract(user_item_matrix, user_item_est) # total errors and keep track of them err = np.sum(np.sum(np.abs(diffs))) sum_errs.append(err) plt.plot(num_latent_feats, 1 - np.array(sum_errs)/df.shape[0]); plt.xlabel(&#39;Number of Latent Features&#39;); plt.ylabel(&#39;Accuracy&#39;); plt.title(&#39;Accuracy vs. Number of Latent Features&#39;); . Train test split . df_train = df.head(40000) df_test = df.tail(5993) def create_test_and_train_user_item(df_train, df_test): &#39;&#39;&#39; INPUT: df_train - training dataframe df_test - test dataframe OUTPUT: user_item_train - a user-item matrix of the training dataframe (unique users for each row and unique articles for each column) user_item_test - a user-item matrix of the testing dataframe (unique users for each row and unique articles for each column) test_idx - all of the test user ids as list test_arts - all of the test article ids as list &#39;&#39;&#39; # Your code here user_item_train = create_user_item_matrix(df_train) user_item_test = create_user_item_matrix(df_test) test_idx = list(set(user_item_test.index)) test_arts = list(set(user_item_test.columns)) return user_item_train, user_item_test, test_idx, test_arts user_item_train, user_item_test, test_idx, test_arts = create_test_and_train_user_item(df_train, df_test) . print(len(test_idx) - len(user_item_test)) print(len(test_arts)) print(len(user_item_test)) print(len(test_arts) - len(user_item_test.columns)) . 0 574 682 0 . Now use the user_item_train dataset from above to find U, S, and V transpose using SVD. Then find the subset of rows in the user_item_test dataset that you can predict using this matrix decomposition with different numbers of latent features to see how many features makes sense to keep based on the accuracy on the test data. . u_train, s_train, vt_train = np.linalg.svd(user_item_train) # fit svd similar to above then use the cells below . # decomposition to predict on test data # CODE REFERENCE: https://github.com/nicovillena/recommendations-with-ibm/blob/master/Recommendations_with_IBM.ipynb # train idx and articles train_idx = np.array(user_item_train.index) train_arts = np.array(user_item_train.columns) # intersection between test idx, articles and train idx, articles #test_idx_int = np.array(set(test_idx).intersection(set(train_idx))).sort() #test_arts_int = np.array(set(test_arts).intersection(set(train_arts))).sort() test_idx_int = np.intersect1d(test_idx, train_idx) test_arts_int = np.intersect1d(test_arts, train_arts) # user and article positions of test subset in training and vice versa train_indexes = np.where(np.in1d(train_idx, test_idx_int))[0] train_articles = np.where(np.in1d(train_arts, test_arts_int))[0] test_indexes = np.where(np.in1d(test_idx, test_idx_int))[0] test_articles = np.where(np.in1d(test_arts, test_arts_int))[0] # subset of user_item matrix containing train and test set u_item_test = user_item_test.iloc[test_indexes,:] print(u_item_test.shape) u_item_train = user_item_train.iloc[train_indexes, train_articles] print(u_item_train.shape) . (20, 574) (20, 574) . latent_feats = np.arange(10,570,10) sum_errors = [] # iterate to find best number of latent_feats for k in latent_feats: # restructure with k latent features s_new, u_new, vt_new = np.diag(s_train[:k]), u_train[:, :k], vt_train[:k, :] # restructure test with k latent features s_test_new, u_test_new, vt_test_new = s_new, u_new[train_indexes,:], vt_new[:,train_articles] # take dot product u_item_test_set_pred = np.around(np.dot(np.dot(u_test_new, s_test_new), vt_test_new)) # compute error for each prediction to actual value error = np.subtract(u_item_test, u_item_test_set_pred) # total errors and keep track of them total_error = np.sum(np.sum(np.abs(error))) sum_errors.append(total_error) # Plot dim = u_item_test.shape[0] * u_item_test.shape[1] plt.plot(latent_feats, 1 - np.array(sum_errors) / dim); plt.xlabel(&#39;Number of Latent Features&#39;); plt.ylabel(&#39;Accuracy&#39;); plt.title(&#39;Accuracy vs. Number of Latent Features&#39;); . Conclusion . We can see that as the number of latent feature increases, the accuracy decreases. Which is a sign of overfitting since this is a plot of the test set. This can be explained by the small number of users who have both testing and training datasets. We conclude that it is robust enough to decide if our model is ready for deployment. To fix this problem, we can collect more data or use regularizations. In addition we can perform an online A/B testing to measure whether rank based recommendation system or matrix recommendation performs better. Note that our accuracy metrics may not be the best measure of our performance since it is so skewed that by just hard coding, we can correctly guess all except 20. Better metric to use may be precision/recall. . Make html notebook . from subprocess import call call([&#39;python&#39;, &#39;-m&#39;, &#39;nbconvert&#39;, &#39;Recommendations_with_IBM.ipynb&#39;]) . 0 .",
            "url": "https://leejaeka.github.io/jaekangai/python/collaborative%20recommendation/eda/ibm/svm/matrix%20factorization/udacity/2021/01/15/Recommendations.html",
            "relUrl": "/python/collaborative%20recommendation/eda/ibm/svm/matrix%20factorization/udacity/2021/01/15/Recommendations.html",
            "date": " ‚Ä¢ Jan 15, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "My friend likes to bike üö¥",
            "content": "WEB-APP HERE . Note that the web-app takes about a min to load! . GIT REPOSITORY HERE . This is just codes I used on the notebook to understand and clean the data. The data comes from my friend who likes to bike. . import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.metrics import r2_score, mean_squared_error import seaborn as sns from sklearn.utils import shuffle import plotly.graph_objs as go import plotly.express as px %matplotlib inline . df = pd.read_csv (&#39;data/activities.csv&#39;) orig = pd.read_csv(&#39;data/activities.csv&#39;) df = shuffle(df) print(&quot;number of rows: &quot;+ str(len(df))) df.head(1) . number of rows: 142 . Activity ID Activity Date Activity Name Activity Type Activity Description Elapsed Time Distance Relative Effort Commute Activity Gear ... Gear Precipitation Probability Precipitation Type Cloud Cover Weather Visibility UV Index Weather Ozone translation missing: en-US.lib.export.portability_exporter.activities.horton_values.jump_count translation missing: en-US.lib.export.portability_exporter.activities.horton_values.total_grit translation missing: en-US.lib.export.portability_exporter.activities.horton_values.avg_flow . 47 723876967 | Sep 24, 2016, 10:54:54 PM | Afternoon Ride | Ride | NaN | 12735 | 29.55 | NaN | False | Vilano Aluminum Road Bike 21 Speed Shimano | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 rows √ó 77 columns . quater_nulls = list(df.columns[df.isnull().sum() &lt;= 0.25*len(df)]) #quater_nulls . df = df[quater_nulls] df.columns . Index([&#39;Activity ID&#39;, &#39;Activity Date&#39;, &#39;Activity Name&#39;, &#39;Activity Type&#39;, &#39;Elapsed Time&#39;, &#39;Distance&#39;, &#39;Commute&#39;, &#39;Activity Gear&#39;, &#39;Filename&#39;, &#39;Athlete Weight&#39;, &#39;Bike Weight&#39;, &#39;Elapsed Time.1&#39;, &#39;Moving Time&#39;, &#39;Distance.1&#39;, &#39;Max Speed&#39;, &#39;Elevation Gain&#39;, &#39;Elevation Low&#39;, &#39;Elevation High&#39;, &#39;Max Grade&#39;, &#39;Average Grade&#39;, &#39;Average Watts&#39;, &#39;Calories&#39;, &#39;Commute.1&#39;, &#39;Bike&#39;], dtype=&#39;object&#39;) . Going to remove discard = [&#39;Activity Name&#39;, &#39;Activity ID&#39;, &#39;Commute&#39;, &#39;Filename&#39;, &#39;Commute.1&#39;,&#39;Distance&#39;, &#39;Elapsed Time.1&#39;, &#39;Bike&#39;] Because not information or repetitive. . discard = [&#39;Activity Name&#39;, &#39;Activity ID&#39;, &#39;Commute&#39;, &#39;Filename&#39;, &#39;Commute.1&#39;,&#39;Distance&#39;, &#39;Elapsed Time.1&#39;, &#39;Bike&#39;] df = df.drop(discard, axis = 1) df.head(1) . Activity Date Activity Type Elapsed Time Activity Gear Athlete Weight Bike Weight Moving Time Distance.1 Max Speed Elevation Gain Elevation Low Elevation High Max Grade Average Grade Average Watts Calories . 47 Sep 24, 2016, 10:54:54 PM | Ride | 12735 | Vilano Aluminum Road Bike 21 Speed Shimano | 63.502899 | 11.0 | 7946.0 | 29549.900391 | 12.3 | 11.1737 | 1.2 | 13.2 | 38.299999 | -0.002369 | 53.709999 | 475.859314 | . df.describe() . Elapsed Time Athlete Weight Bike Weight Moving Time Distance.1 Max Speed Elevation Gain Elevation Low Elevation High Max Grade Average Grade Average Watts Calories . count 142.000000 | 128.000000 | 121.000000 | 142.000000 | 142.000000 | 136.000000 | 137.000000 | 135.000000 | 135.000000 | 136.000000 | 142.000000 | 126.000000 | 132.000000 | . mean 8307.028169 | 65.011994 | 8.886777 | 5964.485915 | 34740.822462 | 13.735294 | 262.614366 | 26.635556 | 95.854814 | 27.179412 | 0.271421 | 111.032301 | 764.836008 | . std 5371.122774 | 5.263799 | 1.400711 | 3368.764442 | 21521.125565 | 3.903418 | 270.558590 | 47.756425 | 103.870238 | 15.398105 | 3.187175 | 28.786978 | 489.682759 | . min 204.000000 | 55.000000 | 7.500000 | 182.000000 | 0.000000 | 0.000000 | 0.000000 | -18.000000 | 6.900000 | 0.000000 | -0.752807 | 49.716900 | 26.208254 | . 25% 3414.500000 | 60.000000 | 7.500000 | 2949.000000 | 17527.250488 | 11.800000 | 62.490898 | -1.000000 | 21.850000 | 14.350000 | -0.003579 | 91.731985 | 383.654442 | . 50% 8070.500000 | 68.000000 | 9.000000 | 6047.500000 | 31560.699219 | 13.700000 | 166.636993 | 0.900000 | 101.099998 | 22.300000 | 0.000000 | 114.522282 | 673.522827 | . 75% 11301.000000 | 68.000000 | 11.000000 | 7957.250000 | 50011.325195 | 15.300000 | 358.088989 | 72.400002 | 125.799999 | 42.899999 | 0.010629 | 130.717503 | 1066.323883 | . max 28317.000000 | 70.000000 | 11.000000 | 16708.000000 | 91705.296875 | 36.299999 | 1455.640015 | 382.299988 | 1092.099976 | 50.000000 | 37.947071 | 182.307999 | 2375.330322 | . sns.heatmap(df.corr(), annot=True, fmt=&#39;.2f&#39;, ax = plt.figure(figsize = (15,10)).gca()) . &lt;AxesSubplot:&gt; . Interesting Correlations: . Elevation Low,High with Athelete Weight, bike weight | Calories and Moving Time and Distance.1 and Elevation Gain | Elevation High, Low and Average Grade | Elevation Gain with Elapsed time, Moving Time, Distance | . df.hist(ax = plt.figure(figsize = (15,20)).gca()); . &lt;ipython-input-141-c15b20fe6ff7&gt;:1: UserWarning: To output multiple subplots, the figure containing the passed axes is being cleared df.hist(ax = plt.figure(figsize = (15,20)).gca()); . Categorical Columns . cat_df = df.select_dtypes(include=[&#39;object&#39;]) #choose categorical columns cat_df . Activity Date Activity Type Activity Gear . 47 Sep 24, 2016, 10:54:54 PM | Ride | Vilano Aluminum Road Bike 21 Speed Shimano | . 8 May 23, 2015, 10:26:06 PM | Ride | NaN | . 94 Sep 29, 2018, 4:19:38 PM | Ride | Gusto | . 55 Mar 22, 2017, 3:44:50 PM | Ride | Kestrel 200 SCI Older Road Bike | . 40 Apr 16, 2016, 5:57:42 PM | Ride | Vilano Aluminum Road Bike 21 Speed Shimano | . ... ... | ... | ... | . 30 Mar 16, 2016, 6:25:36 PM | Ride | Vilano Aluminum Road Bike 21 Speed Shimano | . 66 Jun 23, 2017, 11:25:10 PM | Ride | Kestrel 200 SCI Older Road Bike | . 62 May 9, 2017, 10:33:30 PM | Ride | Kestrel 200 SCI Older Road Bike | . 91 Aug 22, 2018, 9:34:35 PM | Ride | Gusto | . 35 Mar 23, 2016, 5:35:32 AM | Run | NaN | . 142 rows √ó 3 columns . Lets clean these up üßπ . time = df[&#39;Activity Date&#39;].astype(&#39;datetime64[ns]&#39;) yr,mon,d,h = [],[],[],[] for i in time: yr.append(i.year) mon.append(i.month) d.append(i.day) h.append(i.hour) len(yr) time.head(4) df[&#39;Year&#39;] = yr df[&#39;Month&#39;] = mon df[&#39;Day&#39;] = d df[&#39;Hour&#39;] = h . df = df.drop([&#39;Activity Date&#39;], axis=1) # Drop original Date value df.head(3) . Activity Type Elapsed Time Activity Gear Athlete Weight Bike Weight Moving Time Distance.1 Max Speed Elevation Gain Elevation Low Elevation High Max Grade Average Grade Average Watts Calories Year Month Day Hour . 47 Ride | 12735 | Vilano Aluminum Road Bike 21 Speed Shimano | 63.502899 | 11.0 | 7946.0 | 29549.900391 | 12.3 | 11.173700 | 1.2 | 13.200000 | 38.299999 | -0.002369 | 53.709999 | 475.859314 | 2016 | 9 | 24 | 22 | . 8 Ride | 11734 | NaN | 56.699001 | NaN | 10057.0 | 59956.300781 | 14.6 | 825.666992 | -2.4 | 101.099998 | 46.500000 | 0.079558 | 130.302002 | 1461.148682 | 2015 | 5 | 23 | 22 | . 94 Ride | 4696 | Gusto | 68.000000 | 7.5 | 4127.0 | 27227.500000 | 14.0 | 158.414581 | 75.0 | 158.199997 | 11.000000 | 0.235424 | 109.483162 | 580.913513 | 2018 | 9 | 29 | 16 | . print(&quot;Unique Activity Gear values: &quot; + str(df[&#39;Activity Gear&#39;].unique())) print(&quot;Unique Activity Gear values: &quot; + str(df[&#39;Activity Type&#39;].unique())) . Unique Activity Gear values: [&#39;Gusto&#39; &#39;Kestrel 200 SCI Older Road Bike&#39; nan &#39;Vilano Aluminum Road Bike 21 Speed Shimano&#39; &#39;Fixie&#39;] Unique Activity Gear values: [&#39;Ride&#39; &#39;Hike&#39; &#39;Run&#39; &#39;Workout&#39; &#39;Walk&#39;] . def create_dummy_df(df, cat_cols, dummy_na=False): &#39;&#39;&#39; INPUT: df - pandas dataframe with categorical variables you want to dummy cat_cols - list of strings that are associated with names of the categorical columns dummy_na - Bool whether you want to dummy NA values or not OUTPUT: df - new dataframe with following characteristics: 1. contains all columns that were not specified as categorical 2. removes all the original columns in cat_cols 3. dummy columns for each of the categorical columns in cat_cols 4. use a prefix of the column name with an underscore (_) for separating 5. if dummy_na is True - it also contains dummy columns for NaN values &#39;&#39;&#39; for col in cat_cols: try: df = pd.concat([df.drop(col, axis=1), pd.get_dummies(df[col], prefix=col, prefix_sep=&#39;_&#39;, drop_first=True, dummy_na=dummy_na)], axis=1) except: continue return df df = create_dummy_df(df, [&#39;Activity Type&#39;], dummy_na = True) df.head(3) . Elapsed Time Activity Gear Athlete Weight Bike Weight Moving Time Distance.1 Max Speed Elevation Gain Elevation Low Elevation High ... Calories Year Month Day Hour Activity Type_Ride Activity Type_Run Activity Type_Walk Activity Type_Workout Activity Type_nan . 47 12735 | Vilano Aluminum Road Bike 21 Speed Shimano | 63.502899 | 11.0 | 7946.0 | 29549.900391 | 12.3 | 11.173700 | 1.2 | 13.200000 | ... | 475.859314 | 2016 | 9 | 24 | 22 | 1 | 0 | 0 | 0 | 0 | . 8 11734 | NaN | 56.699001 | NaN | 10057.0 | 59956.300781 | 14.6 | 825.666992 | -2.4 | 101.099998 | ... | 1461.148682 | 2015 | 5 | 23 | 22 | 1 | 0 | 0 | 0 | 0 | . 94 4696 | Gusto | 68.000000 | 7.5 | 4127.0 | 27227.500000 | 14.0 | 158.414581 | 75.0 | 158.199997 | ... | 580.913513 | 2018 | 9 | 29 | 16 | 1 | 0 | 0 | 0 | 0 | . 3 rows √ó 23 columns . Null Values . no_nulls = list(df.columns[df.isnull().sum() != 0]) no_nulls . [&#39;Activity Gear&#39;, &#39;Athlete Weight&#39;, &#39;Bike Weight&#39;, &#39;Max Speed&#39;, &#39;Elevation Gain&#39;, &#39;Elevation Low&#39;, &#39;Elevation High&#39;, &#39;Max Grade&#39;, &#39;Average Watts&#39;, &#39;Calories&#39;] . Based on their histogram, it seem like a good idea to . Imputation on median: [Athlete Weight, Bike Weight, Elevation Low, Elevation High] | Imputation on mean: [Elevation Gain, Average Watts, Calories, Max Speed, Max Grade] | . fill_mean = lambda col: col.fillna(col.mean()) # function for imputating mean fill_median = lambda col: col.fillna(col.median()) # function for imputating median # impuation on mean fill_df = df[[&#39;Elevation Gain&#39;, &#39;Average Watts&#39;, &#39;Calories&#39;, &#39;Max Speed&#39;, &#39;Max Grade&#39;]].apply(fill_mean, axis=0) fill_df = pd.concat([fill_df, df.drop([&#39;Elevation Gain&#39;, &#39;Average Watts&#39;, &#39;Calories&#39;, &#39;Max Speed&#39;, &#39;Max Grade&#39;], axis=1)], axis=1) # imputation on median fill_df_med = df[[&#39;Athlete Weight&#39;, &#39;Bike Weight&#39;, &#39;Elevation Low&#39;, &#39;Elevation High&#39;]].apply(fill_median, axis=0) filled_df = pd.concat([fill_df.drop([&#39;Athlete Weight&#39;, &#39;Bike Weight&#39;, &#39;Elevation Low&#39;, &#39;Elevation High&#39;], axis = 1), fill_df_med], axis=1) # Alternative solution to null values by dropping all dropped_df = df.dropna() filled_df.head(2) . Elevation Gain Average Watts Calories Max Speed Max Grade Elapsed Time Activity Gear Moving Time Distance.1 Average Grade ... Hour Activity Type_Ride Activity Type_Run Activity Type_Walk Activity Type_Workout Activity Type_nan Athlete Weight Bike Weight Elevation Low Elevation High . 8 825.666992 | 130.302002 | 1461.148682 | 14.600000 | 46.500000 | 11734 | NaN | 10057.0 | 59956.300781 | 0.079558 | ... | 22 | 1 | 0 | 0 | 0 | 0 | 56.699001 | 9.0 | -2.4 | 101.099998 | . 28 41.823601 | 128.156006 | 1058.558350 | 19.200001 | 24.700001 | 8448 | Vilano Aluminum Road Bike 21 Speed Shimano | 7408.0 | 53329.601562 | -0.015939 | ... | 15 | 1 | 0 | 0 | 0 | 0 | 60.000000 | 11.0 | -1.0 | 42.200001 | . 2 rows √ó 23 columns . dropped_df.head(2) . Elapsed Time Activity Gear Athlete Weight Bike Weight Moving Time Distance.1 Max Speed Elevation Gain Elevation Low Elevation High ... Calories Year Month Day Hour Activity Type_Ride Activity Type_Run Activity Type_Walk Activity Type_Workout Activity Type_nan . 101 11327 | Gusto | 68.000000 | 7.5 | 7993.0 | 54209.500000 | 11.5 | 240.664948 | 74.800003 | 113.900002 | ... | 975.611206 | 2019 | 5 | 15 | 21 | 1 | 0 | 0 | 0 | 0 | . 44 5335 | Vilano Aluminum Road Bike 21 Speed Shimano | 67.131599 | 11.0 | 5038.0 | 38688.300781 | 11.8 | 24.196800 | 0.000000 | 13.900000 | ... | 770.871704 | 2016 | 8 | 29 | 17 | 1 | 0 | 0 | 0 | 0 | . 2 rows √ó 23 columns . filled_df = filled_df.dropna() #Note: can change this to na -&gt; no bike (on foot) . no_nulls = list(filled_df.columns[filled_df.isnull().sum() != 0]) no_nulls_2 = list(dropped_df.columns[dropped_df.isnull().sum() != 0]) assert(no_nulls_2 == []) assert(no_nulls == []) . Linear Regression . y = filled_df[&#39;Distance.1&#39;] X = filled_df.drop([&#39;Distance.1&#39;], axis = 1) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, random_state=42) lm_model = LinearRegression(normalize=True) . lm_model.fit(X_train, y_train) . LinearRegression(normalize=True) . test_pred = lm_model.predict(X_test) train_pred = lm_model.predict(X_train) r2_test = r2_score(y_test, test_pred) r2_train = r2_score(y_train, train_pred) print(&quot;test r2: &quot;+str(r2_test)) print(&quot;train r2: &quot;+str(r2_train)) . test r2: 0.6550060428615112 train r2: 0.9398003894033616 . bike_df = filled_df.drop([&#39;Bike Weight&#39;], axis=1) # Drop bike weight so it doesn&#39;t cheat y = bike_df[&#39;Activity Gear&#39;] X = bike_df.drop([&#39;Activity Gear&#39;], axis = 1) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, random_state=42) . Random Forests . from sklearn.ensemble import RandomForestClassifier rnd_clf = RandomForestClassifier(n_estimators=2000, max_leaf_nodes=32, n_jobs=-1) rnd_clf.fit(X_train, y_train) . RandomForestClassifier(max_leaf_nodes=32, n_estimators=2000, n_jobs=-1) . y_test_pred = rnd_clf.predict(X_test) test_acc = np.sum(y_test_pred == y_test)/len(y_test) print(&quot;test accuracy: &quot;+str(test_acc)) . test accuracy: 0.8421052631578947 . from sklearn.preprocessing import LabelEncoder #https://stackoverflow.com/questions/65549588/shap-treeexplainer-for-randomforest-multiclass-what-is-shap-valuesi labels = [ &quot;Fixie&quot;, &quot;Kestrel 200 SCI Older Road Bike&quot;, &quot;Vilano Aluminum Road Bike 21 Speed Shimano&quot;, &quot;Gusto&quot;, ] le = LabelEncoder() z = le.fit_transform(labels) encoding_scheme = dict(zip(z, labels)) print(encoding_scheme) . {0: &#39;Fixie&#39;, 2: &#39;Kestrel 200 SCI Older Road Bike&#39;, 3: &#39;Vilano Aluminum Road Bike 21 Speed Shimano&#39;, 1: &#39;Gusto&#39;} . sum(y == &#39;Kestrel 200 SCI Older Road Bike&#39;) . 39 . import shap explainer = shap.TreeExplainer(rnd_clf) shap_values = explainer.shap_values(X) # SHAP plot for Gusto shap.summary_plot(shap_values[1], X) . shap.summary_plot(shap_values[2], X) . shap.summary_plot(shap_values[3], X) . Make it harder for computer to guess . bike_df = filled_df.drop([&#39;Bike Weight&#39;, &#39;Year&#39;, &#39;Athlete Weight&#39;], axis=1) # Drop bike weight so it doesn&#39;t cheat y = bike_df[&#39;Activity Gear&#39;] X = bike_df.drop([&#39;Activity Gear&#39;], axis = 1) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, random_state=42) . rnd_clf = RandomForestClassifier(n_estimators=2000, max_leaf_nodes=32, n_jobs=-1) rnd_clf.fit(X_train, y_train) . RandomForestClassifier(max_leaf_nodes=32, n_estimators=2000, n_jobs=-1) . y_test_pred = rnd_clf.predict(X_test) test_acc = np.sum(y_test_pred == y_test)/len(y_test) print(&quot;test accuracy: &quot;+str(test_acc)) . test accuracy: 0.5263157894736842 . explainer = shap.TreeExplainer(rnd_clf) shap_values = explainer.shap_values(X) # Gustov shap.summary_plot(shap_values[1], X) . shap.dependence_plot(&#39;Max Grade&#39;, shap_values[1], X, interaction_index=&#39;Elevation Low&#39;) . What the heck makes Elevation Low a good guessing tool? Maybe my friend liked more mountains with certain bikes . # Plotly tests import plotly.express as px df_i = px.data.iris() features = [&quot;sepal_width&quot;, &quot;sepal_length&quot;, &quot;petal_width&quot;, &quot;petal_length&quot;] fig = px.scatter_matrix( df_i, dimensions=features, color=&quot;species&quot; ) fig.update_traces(diagonal_visible=False) fig.show() .",
            "url": "https://leejaeka.github.io/jaekangai/python/jupyter/crisp-dm/bootstrap/plotly/flask/pca/2021/01/05/bike.html",
            "relUrl": "/python/jupyter/crisp-dm/bootstrap/plotly/flask/pca/2021/01/05/bike.html",
            "date": " ‚Ä¢ Jan 5, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "LoL Prediction S11 üó°Ô∏è",
            "content": ". image src: Riot Games Introduction . Riot Games brings massive changes to their game &#39;League of Legend&#39; every year. This year, they changed their item system, drastically changing their game ecosystem. It has been few months since the big update and now players have fully adapted to the changes. Let&#39;s take a look at what happened to the ecosystem and what is the best team composition now. . Find out what are the most popular champions now. | Find out which team composition is the best. | Compare Season 10 and pre-Season 11. How did the item changes impact the game? | . The dataset . The data we are going to use is a csv file obtained from scraping op.gg which is a website with League of Legend statistics. If you are interested you can visit here. The dataset consists of 2901 ranked matches from Korea(WWW), North America(NA), Eastern Europe(EUNE), and Western Europe(EUW) servers. It has which team won the match, the total time of the match, blue team composition and red team composition. Note that only the high elo games were added this includes Challenger, Grand Master, Master and sometimes even High Diamonds. Note that there are 153 total unique champions with &#39;Rell&#39; as the latest addition. Duplicate games have been removed. . import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.metrics import r2_score, mean_squared_error import seaborn as sns from sklearn.utils import shuffle %matplotlib inline . df = pd.read_csv (&#39;data/s11.csv&#39;) df = shuffle(df) df.head() . result server team_1__001 team_1__002 team_1__003 team_1__004 team_1__005 team_2__001 team_2__002 team_2__003 team_2__004 team_2__005 timestamp game_length . 1265 Defeat | www | Camille | Hecarim | Neeko | Aphelios | Sett | Rumble | Kayn | Twisted Fate | Miss Fortune | Leona | 2020-12-07 18:48:25 | 34m 23s | . 1958 Defeat | eune | Kennen | Rengar | Kassadin | Miss Fortune | Bard | Kayle | Graves | Fiora | Caitlyn | Thresh | 2020-12-18 18:26:55 | 16m 12s | . 1877 Victory | euw | Mordekaiser | Olaf | Zoe | Jhin | Alistar | Shen | Hecarim | LeBlanc | Aphelios | Galio | 2020-12-30 08:58:13 | 34m 27s | . 778 Victory | www | Aatrox | Elise | Lucian | Miss Fortune | Pantheon | Camille | Graves | Zoe | Jhin | Leona | 2020-12-29 21:49:55 | 18m 56s | . 2591 Defeat | na | Poppy | Kayn | Akali | Senna | Braum | Volibear | Olaf | Yone | Twisted Fate | Janna | 2020-11-10 07:38:39 | 31m 15s | . Data Cleaning . Change game_length to continuous variable | Clean null values and uninformative columns | Change categorical variables to dummy variables | import re date_str = df.game_length for i in range(len(date_str)): if type(date_str[i]) == str: p = re.compile(&#39; d*&#39;) min = float(p.findall(date_str[i][:2])[0]) temp = p.findall(date_str[i][-3:]) for j in temp: if j != &#39;&#39;: sec = float(j) break date_str[i] = (60*min+sec) else: date_str[i] = date_str[i] # print(date_str[i]) # print(len(date_str)) # remove timestamp since it does not affect the game df = df.drop([&#39;timestamp&#39;], axis=1) . df.describe() . result server team_1__001 team_1__002 team_1__003 team_1__004 team_1__005 team_2__001 team_2__002 team_2__003 team_2__004 team_2__005 game_length . count 2901 | 2901 | 2901 | 2901 | 2901 | 2901 | 2901 | 2901 | 2901 | 2901 | 2901 | 2901 | 2901.0 | . unique 2 | 4 | 96 | 62 | 102 | 70 | 56 | 95 | 58 | 102 | 72 | 63 | 235.0 | . top Defeat | www | Camille | Graves | Akali | Jhin | Leona | Camille | Graves | Yone | Kai&#39;Sa | Leona | 1818.0 | . freq 2271 | 1592 | 305 | 581 | 235 | 590 | 355 | 266 | 504 | 226 | 568 | 381 | 100.0 | . Most popular champions . Camille(Top): 19.68% pick rate | Graves(Jg): 37.4% pick rate | Akali/Yone(Mid): 15.89% pick rate combined | Jhin/Kai&#39;sa(Adc): 39.92% pick rate combined | Leona(Supp): 25.37% pick rate | . Notes: . The result is very skewed because there are 2271 Red Team win compared to only 630 Blue Team wins | There are in total 2901 games and more than half of it is from Korean server | . no_nulls = set(df.columns[df.isnull().sum()==0]) print(no_nulls) . {&#39;result&#39;, &#39;server&#39;, &#39;team_1__004&#39;, &#39;team_2__003&#39;, &#39;team_1__001&#39;, &#39;team_2__005&#39;, &#39;team_1__003&#39;, &#39;team_2__001&#39;, &#39;game_length&#39;, &#39;team_1__002&#39;, &#39;team_1__005&#39;, &#39;team_2__004&#39;, &#39;team_2__002&#39;} . So there are no null values which is good! . cat_cols = [&#39;result&#39;, &#39;server&#39;, &#39;team_1__004&#39;, &#39;team_2__003&#39;, &#39;team_1__001&#39;, &#39;team_2__005&#39;, &#39;team_1__003&#39;, &#39;team_2__001&#39;,&#39;team_1__002&#39;, &#39;team_1__005&#39;, &#39;team_2__004&#39;, &#39;team_2__002&#39;] def create_dummy_df(df, cat_cols): &#39;&#39;&#39; INPUT: df - pandas dataframe with categorical variables you want to dummy cat_cols - list of strings that are associated with names of the categorical columns OUTPUT: df - new dataframe with following characteristics: 1. contains all columns that were not specified as categorical 2. removes all the original columns in cat_cols 3. dummy columns for each of the categorical columns in cat_cols 4. Use a prefix of the column name with an underscore (_) for separating &#39;&#39;&#39; for col in cat_cols: try: df = pd.concat([df.drop(col, axis=1), pd.get_dummies(df[col], prefix=col, prefix_sep=&#39;_&#39;, drop_first=True)], axis=1) except: continue return df . df = create_dummy_df(df, cat_cols) . max_time = max(df[&#39;game_length&#39;]) df=pd.concat([df.drop(&#39;game_length&#39;, axis=1), (df[&#39;game_length&#39;]/max_time)], axis=1) . df.head(10) . result_Victory server_euw server_na server_www team_1__004_Akali team_1__004_Anivia team_1__004_Annie team_1__004_Aphelios team_1__004_Ashe team_1__004_Aurelion Sol ... team_2__002_Udyr team_2__002_Urgot team_2__002_Vi team_2__002_Volibear team_2__002_Warwick team_2__002_Wukong team_2__002_Xin Zhao team_2__002_Zac team_2__002_Zed game_length . 1265 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.700272 | . 1958 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.329939 | . 1877 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.701629 | . 778 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.385608 | . 2591 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.636456 | . 655 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.517651 | . 1089 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.700272 | . 1221 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.617108 | . 1480 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.419212 | . 1791 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.556687 | . 10 rows √ó 771 columns . The data is ready for modelling. . Linear Regression . y = df[&#39;result_Victory&#39;] X = df.drop([&#39;result_Victory&#39;], axis = 1) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42) lm_model = LinearRegression(normalize=True) . lm_model.fit(X_train, y_train) . LinearRegression(normalize=True) . test_pred = lm_model.predict(X_test) train_pred = lm_model.predict(X_train) r2_test = r2_score(y_test, test_pred) r2_train = r2_score(y_train, train_pred) print(&quot;test r2: &quot;+str(r2_test)) print(&quot;train r2: &quot;+str(r2_train)) . test r2: -2.7134011717466985e+28 train r2: 0.25972262531839096 . Clearly, linear regression is a poor model for this problem haha. Makes sense since we only have discrete fields except game_length. . def coef_weights(coefficients, X_train): &#39;&#39;&#39; INPUT: coefficients - the coefficients of the linear model X_train - the training data, so the column names can be used OUTPUT: coefs_df - a dataframe holding the coefficient, estimate, and abs(estimate) Provides a dataframe that can be used to understand the most influential coefficients in a linear model by providing the coefficient estimates along with the name of the variable attached to the coefficient. &#39;&#39;&#39; coefs_df = pd.DataFrame() coefs_df[&#39;est_int&#39;] = X_train.columns coefs_df[&#39;coefs&#39;] = lm_model.coef_ coefs_df[&#39;abs_coefs&#39;] = np.abs(lm_model.coef_) coefs_df = coefs_df.sort_values(&#39;abs_coefs&#39;, ascending=False) return coefs_df . coef_df = coef_weights(lm_model.coef_, X_train) coef_df.head(20) . est_int coefs abs_coefs . 435 team_2__001_Aphelios | 7.451403e+14 | 7.451403e+14 | . 689 team_2__004_Sion | -7.451403e+14 | 7.451403e+14 | . 298 team_2__005_Ornn | 5.132176e+14 | 5.132176e+14 | . 248 team_1__001_Sona | -5.052884e+14 | 5.052884e+14 | . 31 team_1__004_Kindred | -4.224175e+14 | 4.224175e+14 | . 43 team_1__004_Pantheon | 4.184556e+14 | 4.184556e+14 | . 282 team_2__005_Jayce | 4.156125e+14 | 4.156125e+14 | . 635 team_1__005_Yasuo | 3.822057e+14 | 3.822057e+14 | . 595 team_1__005_Ekko | 3.822057e+14 | 3.822057e+14 | . 412 team_1__003_Thresh | -3.822057e+14 | 3.822057e+14 | . 79 team_2__003_Bard | 3.781834e+14 | 3.781834e+14 | . 328 team_2__005_Zoe | -3.772819e+14 | 3.772819e+14 | . 19 team_1__004_Gragas | 3.742793e+14 | 3.742793e+14 | . 548 team_1__002_Leona | -3.660990e+14 | 3.660990e+14 | . 611 team_1__005_Nidalee | 3.660990e+14 | 3.660990e+14 | . 526 team_1__002_Camille | -3.586479e+14 | 3.586479e+14 | . 382 team_1__003_Miss Fortune | 3.339794e+14 | 3.339794e+14 | . 5 team_1__004_Annie | 3.293351e+14 | 3.293351e+14 | . 576 team_1__002_Twitch | 3.012684e+14 | 3.012684e+14 | . 292 team_2__005_Miss Fortune | 2.971517e+14 | 2.971517e+14 | . Recall that 1 = Blue win and 0 = Red win. So positive coefs. here means helpful for the Blue team and negative coefs. means helpful for the Red team. Most of the fields in the top 20 table above, are not something we see often. For example 435-aphelios(top), 689-sion(adc), 248-sona(top) are considered troll. Here are some other findings. . Looks like every lane is somewhat equally important as their appearance in the table above are similiar | Most of these are troll picks negatively affecting its own team&#39;s winrate | Picks that are actually helping team&#39;s winrate: Sion(ADC), Pantheon(ADC), Yasuo(Sup)??, Ekko(Sup)?? | This table raises more questions than answers! | . Random Forests . from sklearn.ensemble import RandomForestClassifier rnd_clf = RandomForestClassifier(n_estimators=2000, max_leaf_nodes=32, n_jobs=-1) rnd_clf.fit(X_train, y_train) . RandomForestClassifier(max_leaf_nodes=32, n_estimators=2000, n_jobs=-1) . y_test_pred = rnd_clf.predict(X_test) test_acc = np.sum(y_test_pred == y_test)/len(y_test) print(&quot;test accuracy: &quot;+str(test_acc)) . test accuracy: 0.8003442340791739 . Wow we went from 0% to 80% accuracy with random forest! . import shap explainer = shap.TreeExplainer(rnd_clf) shap_values = explainer.shap_values(X_test) shap.summary_plot(shap_values[1], X_test) . Interestingly, West Europe tend to win more as Blue team as games are longer. In contrast, Korea tend to win more as Red Team as games gets longer. So there seem to be a trend difference between regions. Furthermore, in general, the shorter the game, blue team wins more for some reason I cannot figure out. . Best/Worst Composition . Best . (Top)Camille,Yone (Jg)Hecarim,Olaf,Twitch (Mid)Akali (Adc)Miss Fortune,Jhin (Sup)Alistar,Janna,Leona | . Worst . (Top)Pantheon,Irelia (Jg)Wukong (Mid)Sylas,Yone | . If we compare this with the official na.op.gg champion rankings, all the best champions listed here are also listed on their website as either tier one or two as well. (Except Twitch and Pantheon). Note that this is just for comparison. Op.gg has million times more data with more regions. Also how they rank these champions are not revealed. . Comparing with S10 . Best team composition Worst team composition Comparisons . The new update caused each roles to impact more evenly to the game&#39;s result | Bottom lane has generally good picks with no worst picks in season 11. | The new update caused more &#39;high risk high reward&#39; champions to win more and &#39;generally good&#39; champions to fall | .",
            "url": "https://leejaeka.github.io/jaekangai/python/jupyter/crisp-dm/league%20of%20legends/linear%20regression/random%20forest/udacity/2020/12/30/lolpredictb.html",
            "relUrl": "/python/jupyter/crisp-dm/league%20of%20legends/linear%20regression/random%20forest/udacity/2020/12/30/lolpredictb.html",
            "date": " ‚Ä¢ Dec 30, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "MOOC certificates üìú",
            "content": "Udacity . Data Scientist Nanodegree | . Coursera . Generative Adversarial Networks (GANs) Specialization | Apply Generative Adversarial Networks (GANs) | Build Better Generative Adversarial Networks (GANs) | Build Basic Generative Adversarial Networks (GANs) | Standford Machine Learning (unofficial) | Convolutional Neural Networks | Sequence Models | Neural Networks and Deep Learning | . Kaggle . Deep Learning | Feature Engineering | Intro to Machine Learning | Intermediate Machine Learning | Machine Learning Explainability | Natural Language Processing | . Udemy . Tableau 20 Advanced Training: Master Tableau in Data Science | Tableau 2020 A-Z: Hands-On Tableau Training for Data Science | Scala and Spark for Big Data and Machine Learning | . .",
            "url": "https://leejaeka.github.io/jaekangai/fastpages/jupyter/certificate/2020/12/30/certificates.html",
            "relUrl": "/fastpages/jupyter/certificate/2020/12/30/certificates.html",
            "date": " ‚Ä¢ Dec 30, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Deep Art Gallery üß†",
            "content": ". This is me when I went to Aachen This is my friend Annan&#39;s puppy, Newbie This is Coquitlam, Canada where I spent my high school years! We used to be forced to do laps around this lake! This is meemaw. How many person in this photo? This was outside my room in Toronto and when I was high This is my handsome friend Truman Hung! It has no idea what to do with my hair lol Styles used . I think you can guess which styles were used on which photos. Except Newbie&#39;s style is by Wassily Kandinsky ‚Äî Composition VII . How it works . Neural Style Transfer works by choosing a content image and a style image and then &#39;drawing&#39; the content image using style of the style image. . In implementation, all we are doing is calculating some derivatives to make a number small as possible. . This is the cost function we are trying to minimize. As $J(GeneratedImage)$ gets smaller, we get the art we want. Think of cost function as distance from our art being beautiful. G is initialized as a random noise image. We will use Adam optimization to compute the gradient. Think of gradient as small step towards prettiness. . . So every iteration, G will be subtracted with gradient of $J(GeneratedImage)$ slowly becoming beautiful. . . Content Cost Function $J_{content}(C,G)$ . $$J_{content}(C,G) = frac{1}{4 times n_H times n_W times n_C} sum _{ text{all entries}} (a^{(C)} - a^{(G)})^2 tag{1} $$ . Here, $a$ stands for activation of the lth layer in our convNet. | $n_H$, $n_W$, $n_C$ is the dimension of the layer. (Height, width, depth). | The constants in front are just for normalization. | . Style Cost Function $J_{style}(S,G)$ . $$J_{style}^{[l]}(S,G) = frac{1}{4 times {n_C}^2 times (n_H times n_W)^2} sum _{i=1}^{n_C} sum_{j=1}^{n_C}(G^{(S)}_{(gram)i,j} - G^{(G)}_{(gram)i,j})^2 tag{2} $$ . The constants in front are just for normalization | The gram is a function that just calculates the correlation between horizontal vectors in the given matrix(which is our depths) | We will calculate gram of activation layer from both content and generated layer for all combinations of depths(i,j). | And this is just one layer. Then we compute for all layers. This is why it takes so long to generate our image. | Note that the picture below &#39;unrolled&#39; a 3d volume into 2d matrix. | As you can see style cost function is less straightforward. &quot;If you don&#39;t understand it, don&#39;t worry about it&quot; - Andrew NG. | . Code . Cred to Tensorflow (see reference) Note that most of the arts generated above were using code from a coursera assignment which is different from codes below showing implementation (same but different transferred model) in tensorflow2. Modified to run on gpu. . content_image = load_img(&#39;images/newby.jpg&#39;) style_image = load_img(&#39;images/kandinsky.jpg&#39;) plt.subplot(1, 2, 1) imshow(content_image, &#39;Content Image&#39;) plt.subplot(1, 2, 2) imshow(style_image, &#39;Style Image&#39;) . def tensor_to_image(tensor): tensor = tensor*255 tensor = np.array(tensor, dtype=np.uint8) if np.ndim(tensor)&gt;3: assert tensor.shape[0] == 1 tensor = tensor[0] return PIL.Image.fromarray(tensor) . Transfer Learning . Choice for the model is VGG19 since it is what was used in the original paper by Leon A. Gatys, Alexander S. Ecker, Matthias Bethge. . content_layers = [&#39;block5_conv2&#39;] style_layers = [&#39;block1_conv1&#39;, &#39;block2_conv1&#39;, &#39;block3_conv1&#39;, &#39;block4_conv1&#39;, &#39;block5_conv1&#39;] num_content_layers = len(content_layers) num_style_layers = len(style_layers) . def vgg_layers(layer_names): &quot;&quot;&quot; Creates a vgg model that returns a list of intermediate output values.&quot;&quot;&quot; # Load our model. Load pretrained VGG, trained on imagenet data vgg = tf.keras.applications.VGG19(include_top=False, weights=&#39;imagenet&#39;) vgg.trainable = False outputs = [vgg.get_layer(name).output for name in layer_names] model = tf.keras.Model([vgg.input], outputs) return model . style_extractor = vgg_layers(style_layers) style_outputs = style_extractor(style_image*255) #Look at the statistics of each layer&#39;s output # for name, output in zip(style_layers, style_outputs): # print(name) # print(&quot; shape: &quot;, output.numpy().shape) # print(&quot; min: &quot;, output.numpy().min()) # print(&quot; max: &quot;, output.numpy().max()) # print(&quot; mean: &quot;, output.numpy().mean()) # print() . Helper functions . def gram_matrix(input_tensor): result = tf.linalg.einsum(&#39;bijc,bijd-&gt;bcd&#39;, input_tensor, input_tensor) input_shape = tf.shape(input_tensor) num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32) return result/(num_locations) . class StyleContentModel(tf.keras.models.Model): def __init__(self, style_layers, content_layers): super(StyleContentModel, self).__init__() self.vgg = vgg_layers(style_layers + content_layers) self.style_layers = style_layers self.content_layers = content_layers self.num_style_layers = len(style_layers) self.vgg.trainable = False def call(self, inputs): &quot;Expects float input in [0,1]&quot; inputs = inputs*255.0 preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs) outputs = self.vgg(preprocessed_input) style_outputs, content_outputs = (outputs[:self.num_style_layers], outputs[self.num_style_layers:]) style_outputs = [gram_matrix(style_output) for style_output in style_outputs] content_dict = {content_name:value for content_name, value in zip(self.content_layers, content_outputs)} style_dict = {style_name:value for style_name, value in zip(self.style_layers, style_outputs)} return {&#39;content&#39;:content_dict, &#39;style&#39;:style_dict} . extractor = StyleContentModel(style_layers, content_layers) results = extractor(tf.constant(content_image)) # print(&#39;Styles:&#39;) # for name, output in sorted(results[&#39;style&#39;].items()): # print(&quot; &quot;, name) # print(&quot; shape: &quot;, output.numpy().shape) # print(&quot; min: &quot;, output.numpy().min()) # print(&quot; max: &quot;, output.numpy().max()) # print(&quot; mean: &quot;, output.numpy().mean()) # print() # print(&quot;Contents:&quot;) # for name, output in sorted(results[&#39;content&#39;].items()): # print(&quot; &quot;, name) # print(&quot; shape: &quot;, output.numpy().shape) # print(&quot; min: &quot;, output.numpy().min()) # print(&quot; max: &quot;, output.numpy().max()) # print(&quot; mean: &quot;, output.numpy().mean()) . style_targets = extractor(style_image)[&#39;style&#39;] content_targets = extractor(content_image)[&#39;content&#39;] . image = tf.Variable(content_image) . def clip_0_1(image): return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0) . opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1) . style_weight=1e-2 content_weight=1e4 . def style_content_loss(outputs): style_outputs = outputs[&#39;style&#39;] content_outputs = outputs[&#39;content&#39;] style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2) for name in style_outputs.keys()]) style_loss *= style_weight / num_style_layers content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) for name in content_outputs.keys()]) content_loss *= content_weight / num_content_layers loss = style_loss + content_loss return loss . @tf.function() def train_step(image): with tf.GradientTape() as tape: outputs = extractor(image) loss = style_content_loss(outputs) grad = tape.gradient(loss, image) opt.apply_gradients([(grad, image)]) image.assign(clip_0_1(image)) . Training . train_step(image) train_step(image) train_step(image) tensor_to_image(image) . Do 1000 iteration and save every 200th iteration image . import time with tf.device(&quot;/gpu:0&quot;): start = time.time() epochs = 10 steps_per_epoch = 100 step = 0 for n in range(epochs): for m in range(steps_per_epoch): step += 1 train_step(image) print(&quot;.&quot;, end=&#39;&#39;) display.clear_output(wait=True) display.display(tensor_to_image(image)) print(&quot;Train step: {}&quot;.format(step)) # save current generated image in the &quot;/output&quot; directory imageio.imwrite(&quot;output/&quot; + str(2*100) + &quot;.png&quot;, tensor_to_image(image)) end = time.time() print(&quot;Total time: {:.1f}&quot;.format(end-start)) . Train step: 1000 Total time: 634.3 . imageio.imwrite(&quot;output/&quot; + str(2*100) + &quot;.png&quot;, tensor_to_image(image)) . Total Variation Loss . I didn&#39;t learn this part so its like magic to me . def high_pass_x_y(image): x_var = image[:,:,1:,:] - image[:,:,:-1,:] y_var = image[:,1:,:,:] - image[:,:-1,:,:] return x_var, y_var . x_deltas, y_deltas = high_pass_x_y(content_image) plt.figure(figsize=(14,10)) plt.subplot(2,2,1) imshow(clip_0_1(2*y_deltas+0.5), &quot;Horizontal Deltas Original&quot;) plt.subplot(2,2,2) imshow(clip_0_1(2*x_deltas+0.5), &quot;Vertical Deltas Original&quot;) x_deltas, y_deltas = high_pass_x_y(image) plt.subplot(2,2,3) imshow(clip_0_1(2*y_deltas+0.5), &quot;Horizontal Deltas Styled&quot;) plt.subplot(2,2,4) imshow(clip_0_1(2*x_deltas+0.5), &quot;Vertical Deltas Styled&quot;) . plt.figure(figsize=(14,10)) sobel = tf.image.sobel_edges(content_image) plt.subplot(1,2,1) imshow(clip_0_1(sobel[...,0]/4+0.5), &quot;Horizontal Sobel-edges&quot;) plt.subplot(1,2,2) imshow(clip_0_1(sobel[...,1]/4+0.5), &quot;Vertical Sobel-edges&quot;) . def total_variation_loss(image): x_deltas, y_deltas = high_pass_x_y(image) return tf.reduce_sum(tf.abs(x_deltas)) + tf.reduce_sum(tf.abs(y_deltas)) . tf.image.total_variation(image).numpy() . array([114173.52], dtype=float32) . total_variation_weight=30 . @tf.function() def train_step(image): with tf.GradientTape() as tape: outputs = extractor(image) loss = style_content_loss(outputs) loss += total_variation_weight*tf.image.total_variation(image) grad = tape.gradient(loss, image) opt.apply_gradients([(grad, image)]) image.assign(clip_0_1(image)) . image = tf.Variable(content_image) . import time with tf.device(&quot;/gpu:0&quot;): start = time.time() epochs = 10 steps_per_epoch = 100 step = 0 for n in range(epochs): for m in range(steps_per_epoch): step += 1 train_step(image) print(&quot;.&quot;, end=&#39;&#39;) display.clear_output(wait=True) display.display(tensor_to_image(image)) print(&quot;Train step: {}&quot;.format(step)) end = time.time() print(&quot;Total time: {:.1f}&quot;.format(end-start)) . Train step: 1000 Total time: 834.3 . file_name = &#39;generated_image.png&#39; imageio.imwrite(&quot;output/&quot; + &#39;generated_image&#39; + &quot;.png&quot;, tensor_to_image(image)) . References: . The Neural Style Transfer algorithm was due to Gatys et al. (2015). The pre-trained network used in this implementation is a VGG network, which is due to Simonyan and Zisserman (2015). The whole code is basically from tensorflow website listed below with little changes(to save images and use gpu) . Leon A. Gatys, Alexander S. Ecker, Matthias Bethge, (2015). A Neural Algorithm of Artistic Style | Harish Narayanan, Convolutional neural networks for artistic style transfer. | DeepLearningAi(Coursera) (2020). Deep Learning Specialization | TensorFlow (2019). Neural style transfer | .",
            "url": "https://leejaeka.github.io/jaekangai/fastpages/jupyter/meme/2020/12/02/Deep-Art-Gallery.html",
            "relUrl": "/fastpages/jupyter/meme/2020/12/02/Deep-Art-Gallery.html",
            "date": " ‚Ä¢ Dec 2, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "So I learned some new algorithms.. üÉè",
            "content": "Introduction . Inception Net . You only look once algorithm . How do you detect multiple cars in a single image? . Object Detection .",
            "url": "https://leejaeka.github.io/jaekangai/fastpages/jupyter/meme/2020/11/27/So-I-learned-some-new-algorithms.html",
            "relUrl": "/fastpages/jupyter/meme/2020/11/27/So-I-learned-some-new-algorithms.html",
            "date": " ‚Ä¢ Nov 27, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "White Wine Quality Exploratory Data Analysis With R",
            "content": "======================================================== . The white wine quality dataset consists of 13 variables, with 4898 observations. Note that the quality was determined by at least three different wine experts. Let us see what makes the best white wine! First we run the summary() function in R and get overwhelmed. . ## X fixed.acidity volatile.acidity citric.acid ## Min. : 1 Min. : 3.800 Min. :0.0800 Min. :0.0000 ## 1st Qu.:1225 1st Qu.: 6.300 1st Qu.:0.2100 1st Qu.:0.2700 ## Median :2450 Median : 6.800 Median :0.2600 Median :0.3200 ## Mean :2450 Mean : 6.855 Mean :0.2782 Mean :0.3342 ## 3rd Qu.:3674 3rd Qu.: 7.300 3rd Qu.:0.3200 3rd Qu.:0.3900 ## Max. :4898 Max. :14.200 Max. :1.1000 Max. :1.6600 ## residual.sugar chlorides free.sulfur.dioxide total.sulfur.dioxide ## Min. : 0.600 Min. :0.00900 Min. : 2.00 Min. : 9.0 ## 1st Qu.: 1.700 1st Qu.:0.03600 1st Qu.: 23.00 1st Qu.:108.0 ## Median : 5.200 Median :0.04300 Median : 34.00 Median :134.0 ## Mean : 6.391 Mean :0.04577 Mean : 35.31 Mean :138.4 ## 3rd Qu.: 9.900 3rd Qu.:0.05000 3rd Qu.: 46.00 3rd Qu.:167.0 ## Max. :65.800 Max. :0.34600 Max. :289.00 Max. :440.0 ## density pH sulphates alcohol ## Min. :0.9871 Min. :2.720 Min. :0.2200 Min. : 8.00 ## 1st Qu.:0.9917 1st Qu.:3.090 1st Qu.:0.4100 1st Qu.: 9.50 ## Median :0.9937 Median :3.180 Median :0.4700 Median :10.40 ## Mean :0.9940 Mean :3.188 Mean :0.4898 Mean :10.51 ## 3rd Qu.:0.9961 3rd Qu.:3.280 3rd Qu.:0.5500 3rd Qu.:11.40 ## Max. :1.0390 Max. :3.820 Max. :1.0800 Max. :14.20 ## quality ## Min. :3.000 ## 1st Qu.:5.000 ## Median :6.000 ## Mean :5.878 ## 3rd Qu.:6.000 ## Max. :9.000 . Fixed.volatile.acidity, citric.acid, fixed.acidity, total.sulfur.dioxide, chlorides, free.sulfur.dioxide, sulphates ‚Äì I have no idea what there are since I‚Äôm not a chemist. The point of this project to explore with data analysis so it is okay. Rest variables are self-explanatory except pH which is how ripe the ingredient used for the wine was. We are most interested in what makes the differences in experts‚Äô wine ratings. . . Univariate Plots Section . *It looks like Majority of our wines are quality between ‚Äò5‚Äô to ‚Äò7‚Äô. There are less than 10 of the highest quality ‚Äò9‚Äô and dataset is normally distributed. . *Let‚Äôs look at some boring histograms of rest of the variables. . . ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. . . Boring as expected. . Some interesting points: . Since the dataset consists of real experts (human in the end), I think majority of their points comes from sweetness(sugar), acidity, alcohol and density. | Sugar had two main peaks at around 2 and 10. I guess majority of winemakers like to make their wine either sweet or not sweet. Most datas were skewed due to very few extreme outliers. | I have created total_acidity variable which does not make any sense but simplifies fixed, volatile and citric acids and free_rate which is proportion of free sulfur dioxide in total sulfur dioxide. Both for small dimension reduction purpose. | . One shot look at all bivariates . ## [1] &quot;residual.sugar&quot; &quot;chlorides&quot; &quot;density&quot; &quot;pH&quot; ## [5] &quot;sulphates&quot; &quot;alcohol&quot; &quot;quality&quot; &quot;total_acid&quot; ## [9] &quot;free_rate&quot; . . We can see that most scatterplots tends to form clusters with some outliers. Most interesting findings are‚Ä¶ . Quality is most correlated with alcohol, density, chlorides . | Alcohol is highly correlated with chloride(-0.82) and sugar(-0.46) . | Sugar, alcohol and density are highly correlated . | Let‚Äôs have a closer look. . ## [1] &quot;(2,4]&quot; &quot;(4,7]&quot; &quot;(7,9]&quot; # 3,4 are ‚ÄúLow‚Äù, 5,6,7 are ‚ÄúMedium‚Äù 8,9 are ‚ÄúHigh‚Äù . . Boxplots of different qualities . So clearly, high quality wines tend to have higher alcohol, lower density and lower chlorides than other qualities of wine. . ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 7 x 5 ## quality med_alcohol med_desity med_chloride n ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 3 10.4 0.994 0.041 20 ## 2 4 10.1 0.994 0.046 163 ## 3 5 9.5 0.995 0.047 1457 ## 4 6 10.5 0.994 0.043 2198 ## 5 7 11.4 0.992 0.037 880 ## 6 8 12 0.992 0.036 175 ## 7 9 12.5 0.990 0.031 5 . We used median here since most variables were severly skewed. We can somewhat guess typical characteristics of particular qualities in this chart. We can also see clear difference between quality 3 and 9 instantly. . Multivariate Plots Section . . Multivariate Section . There‚Äôs a lot going on here. First we can easily confirm that indeed density, alcohol and sugar closely related. . 1. Higher sugar levels cover upper right region while lower sugar level covers lower left region. This means higher density and alcohol, higher the sugar level and vice versa. . 2. Alcohol and density are highly correlated. I suspect this is because alcohol is main source of reducing density since every other ingredients are likely to increase density. . 3. High quality wines have a cluster at the higher alcohol and lower density quadrant. In contrast, low quality wines have a cluster located further towards 1st and 3rd quadrants. Medium quality wines covers both area. . 4. No wines seem to exist lowest alcohol and lowest density region. . Multivariate Linear Regression . ## ## Call: ## lm(formula = quality ~ density + alcohol + chlorides, data = wine) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.5904 -0.5209 -0.0050 0.4832 3.0653 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -21.15016 6.16220 -3.432 0.000604 *** ## density 23.67087 6.07373 3.897 9.86e-05 *** ## alcohol 0.34312 0.01529 22.439 &lt; 2e-16 *** ## chlorides -2.38226 0.55760 -4.272 1.97e-05 *** ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7946 on 4894 degrees of freedom ## Multiple R-squared: 0.1955, Adjusted R-squared: 0.195 ## F-statistic: 396.3 on 3 and 4894 DF, p-value: &lt; 2.2e-16 . Of course this is just a old textbook solution. It is terrible and very time consuming to try different variable combinations. Lets use random forest instead. . Random Forest Classifier on all quality [3,9] . ## randomForest 4.6-14 ## ## Call: ## randomForest(formula = factor(quality) ~ ., data = train, ntree = 500, importance = TRUE) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 3 ## ## OOB estimate of error rate: 31.73% ## Confusion matrix: ## 3 4 5 6 7 8 9 class.error ## 3 0 0 6 5 1 0 0 1.0000000 ## 4 0 24 57 43 2 0 0 0.8095238 ## 5 0 3 757 367 13 1 0 0.3365469 ## 6 0 2 216 1440 103 2 0 0.1832104 ## 7 0 0 12 314 391 3 0 0.4569444 ## 8 0 0 2 45 41 63 0 0.5827815 ## 9 0 0 0 2 3 0 0 1.0000000 . We got an accuracy of 69.4%. It is probably hard even for experts to distinguish quality level difference of 1 perfectly. . RFC(All quality) Interpretation (MeanDecreaseAccuracy, MeanDecreaseGini) . . To explain a little about the plots, MeanDecreaseAccuracy tells us how bad our predictions become if we were to omit that variable completly. MeanDecreaseGini tells us how clean the splits are if that variable was used to split the data. | This had me wonder if I bucket some quality together into only 3 qualities, ‚ÄòHigh‚Äô, ‚ÄòMedium‚Äô, and ‚ÄòLow‚Äô I should be able to increase accuracy by a significant amount. | . Random Forest Classifier on bucketed quality (High, Med, Low) . ## ## Call: ## randomForest(formula = factor(quality.bucket) ~ ., data = train, ntree = 500, importance = TRUE) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 3 ## ## OOB estimate of error rate: 5.59% ## Confusion matrix: ## Low Qual. Med. Qual. High Qual. class.error ## Low Qual. 26 112 0 0.811594203 ## Med. Qual. 11 3610 3 0.003863135 ## High Qual. 0 93 63 0.596153846 . Ok we‚Äôve achieved 94.46% accuracy. . RFC(bucketed) Interpretation (MeanDecreaseAccuracy, MeanDecreaseGini) . . ## predicted ## Low Qual. Med. Qual. High Qual. ## Low Qual. 9 36 0 ## Med. Qual. 7 904 0 ## High Qual. 0 14 10 . 93.82% accuracy on our test set! So it looks like there barely any overfitting and it generalizes really well too. We can also note that alcohol once again is most important by a significant amount. Followed by many acids and density &amp; sugar giving little meanings. . RFC(bucketed) Interpretation 2 (ROC Curve) . https://www.blopig.com/blog/2017/04/a-very-basic-introduction-to-random-forests-using-r/ (Reference) . . ## [[1]] ## [1] 0.8978966 ## ## [[1]] ## [1] 0.8655801 ## ## [[1]] ## [1] 0.9413354 . Red - Low Qual., Green - Med Qual., Blue - High Qual. Here we can see that model is very good at picking out the low quality wines but have harder time picking out high quality wines.In fact, if you recall the 7 level quality model, that model didn‚Äôt predict any sample to be high quality. So we can conclude that bad wines are easy to distinguish but best wines are hard to detect. . Reflection . The white wine set contains information on almost 5000 wine data which all have been evaluated by at least 3 different wine experts. We explored many variables and predicted that since density, chlorides, and alcohol had high correlation, their combination will define the quality of any wine. We also looked at an interesting relationship between chemicals like sugar makes density go up and alcohol go down. . Our random forest quantifier agreed with alcohol being important but disagreed with density and chloride being important. Instead it told us a lot of acids which had little to no correlation were very important. And proceeded to succesfully predict 94.46% on training set and 93.82% on test set. Also note it had harder time distinguishing higher quality wines than lower quality wines by looking at its MeanDecreaseAccuracy. Meaning other factors such as presentation or color may may be necessary divide the ‚Äòbest wine‚Äô from good wines. . In conclusion, as predicted from my idea; acidity, sweetness, alcohol level but density plays a very important factor in determining high quality white wines. So although I‚Äôve never had wine in my life, I can taste one and tell if it is good or not : ) .",
            "url": "https://leejaeka.github.io/jaekangai/2020/11/24/White-Wine-Quality-Exploratory-Data-Analysis-with-R.html",
            "relUrl": "/2020/11/24/White-Wine-Quality-Exploratory-Data-Analysis-with-R.html",
            "date": " ‚Ä¢ Nov 24, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "LoL Prediction S10 üèπ",
            "content": "Introduction . Let&#39;s predict who won the match given team composition and how long game played out . Get dataset . The dataset is a collection of League of Legends High Elo(Challenger, GM, Master, High Diamonds) Ranked games in Season 10, Korea(WWW), North America(NA), Eastern Europe(EUNE), and Western Europe(EUW) servers. These datas were collected from op.gg by web scrapping with python spyder. The latest game was played on Oct.16th on the dataset. In total there are 4028 unique games. Note that I&#39;ve used one-hot encoding hence [99,54,101,73,57,96,52,102,68,52] this list represents number of all unique champions used in each lanes [BlueTop, BlueJG, BlueMid, BlueAdc, BlueSup, RedTop, RedJg, RedMid, RedAdc, RedSup] respectivley. Note that there are in total 151 unique champions with &#39;Samira&#39; as the latest addition. . import pandas as pd df = pd.read_csv(&quot;games.csv&quot;) . Some Setups . import sys assert sys.version_info &gt;= (3, 5) # Scikit-Learn ‚â•0.20 is required import sklearn assert sklearn.__version__ &gt;= &quot;0.20&quot; try: # %tensorflow_version only exists in Colab. %tensorflow_version 2.x except Exception: pass # TensorFlow ‚â•2.0 is required import tensorflow as tf from tensorflow import keras assert tf.__version__ &gt;= &quot;2.0&quot; %load_ext tensorboard # Common imports import numpy as np import os # to make this notebook&#39;s output stable across runs np.random.seed(42) # To plot pretty figures %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt mpl.rc(&#39;axes&#39;, labelsize=14) mpl.rc(&#39;xtick&#39;, labelsize=12) mpl.rc(&#39;ytick&#39;, labelsize=12) # Where to save the figures PROJECT_ROOT_DIR = &quot;.&quot; CHAPTER_ID = &quot;deep&quot; IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, &quot;images&quot;, CHAPTER_ID) os.makedirs(IMAGES_PATH, exist_ok=True) def save_fig(fig_id, tight_layout=True, fig_extension=&quot;png&quot;, resolution=300): path = os.path.join(IMAGES_PATH, fig_id + &quot;.&quot; + fig_extension) print(&quot;Saving figure&quot;, fig_id) if tight_layout: plt.tight_layout() plt.savefig(path, format=fig_extension, dpi=resolution) . The tensorboard extension is already loaded. To reload it, use: %reload_ext tensorboard . df.head(5) # First look at our dataset. Game_length includes some annoying string values instead of time value . game_length mmr result server team_1 team_2 timestamp . 0 25m 38s | NaN | Victory | na | Riven,Nidalee,Galio,Jhin,Pantheon | Camille,Olaf,Cassiopeia,Ezreal,Alistar | 2020-10-13 09:31:42 | . 1 25m 38s | NaN | Defeat | na | Teemo,Nidalee,Lucian,Caitlyn,Senna | Irelia,Hecarim,Cassiopeia,Jinx,Lulu | 2020-10-13 06:00:17 | . 2 25m 38s | NaN | Defeat | na | Malphite,Olaf,Taliyah,Ezreal,Alistar | Sylas,Lillia,Lucian,Senna,Pantheon | 2020-10-13 05:06:45 | . 3 25m 38s | NaN | Defeat | na | Neeko,Shen,Orianna,Kai&#39;Sa,Nautilus | Riven,Hecarim,Cassiopeia,Samira,Morgana | 2020-10-13 04:28:00 | . 4 25m 38s | NaN | Defeat | na | Fiora,Nunu &amp; Willump,Irelia,Jhin,Karma | Renekton,Elise,Kled,Jinx,Morgana | 2020-10-13 04:00:51 | . temp_df = df[[&#39;game_length&#39;, &#39;result&#39;, &#39;team_1&#39;, &#39;team_2&#39;]] # Select only interests blue = temp_df[&#39;team_1&#39;] red = temp_df[&#39;team_2&#39;] n = len(df) blue_champs = [] red_champs = [] for i in range(0,n): blue_champs += [blue[i].split(&#39;,&#39;)] red_champs += [red[i].split(&#39;,&#39;)] top = [] jg = [] mid = [] adc = [] sup = [] for i in range(0, n): top += [blue_champs[i][0]] jg += [blue_champs[i][1]] mid += [blue_champs[i][2]] adc += [blue_champs[i][3]] sup += [blue_champs[i][4]] top_2 = [] jg_2 = [] mid_2 = [] adc_2 = [] sup_2 = [] for i in range(0, n): top_2 += [red_champs[i][0]] jg_2 += [red_champs[i][1]] mid_2 += [red_champs[i][2]] adc_2 += [red_champs[i][3]] sup_2 += [red_champs[i][4]] . data = temp_df.drop(columns=[&#39;team_1&#39;,&#39;team_2&#39;]) # blue team data[&#39;top1&#39;] = top data[&#39;jg1&#39;] = jg data[&#39;mid1&#39;] = mid data[&#39;adc1&#39;] = adc data[&#39;sup1&#39;] = sup # red team data[&#39;top2&#39;] = top_2 data[&#39;jg2&#39;] = jg_2 data[&#39;mid2&#39;] = mid_2 data[&#39;adc2&#39;] = adc_2 data[&#39;sup2&#39;] = sup_2 . data.head(10) . game_length result top1 jg1 mid1 adc1 sup1 top2 jg2 mid2 adc2 sup2 . 0 25m 38s | Victory | Riven | Nidalee | Galio | Jhin | Pantheon | Camille | Olaf | Cassiopeia | Ezreal | Alistar | . 1 25m 38s | Defeat | Teemo | Nidalee | Lucian | Caitlyn | Senna | Irelia | Hecarim | Cassiopeia | Jinx | Lulu | . 2 25m 38s | Defeat | Malphite | Olaf | Taliyah | Ezreal | Alistar | Sylas | Lillia | Lucian | Senna | Pantheon | . 3 25m 38s | Defeat | Neeko | Shen | Orianna | Kai&#39;Sa | Nautilus | Riven | Hecarim | Cassiopeia | Samira | Morgana | . 4 25m 38s | Defeat | Fiora | Nunu &amp; Willump | Irelia | Jhin | Karma | Renekton | Elise | Kled | Jinx | Morgana | . 5 25m 38s | Defeat | Irelia | Karthus | Sylas | Samira | Nautilus | Riven | Kayn | Akali | Miss Fortune | Galio | . 6 25m 38s | Defeat | Galio | Kindred | Syndra | Ezreal | Blitzcrank | Camille | Fiddlesticks | Twisted Fate | Jhin | Morgana | . 7 25m 38s | Defeat | Poppy | Ekko | Sylas | Samira | Blitzcrank | Lucian | Lillia | Lulu | Caitlyn | Alistar | . 8 25m 38s | Defeat | Shen | Lillia | Samira | Lucian | Soraka | Taric | Master Yi | Riven | Ezreal | Lulu | . 9 25m 38s | Defeat | Ornn | Graves | Sylas | Lucian | Alistar | Irelia | Hecarim | Akali | Senna | Leona | . from sklearn.preprocessing import OneHotEncoder #y = pd.get_dummies(data.top1, prefix=&#39;top1&#39;) enc = OneHotEncoder() only_champs = data.drop(columns=[&#39;game_length&#39;, &#39;result&#39;]) only_champs.head(5) only_champs_onehot = enc.fit_transform(only_champs) . enc.get_params() . {&#39;categories&#39;: &#39;auto&#39;, &#39;drop&#39;: None, &#39;dtype&#39;: numpy.float64, &#39;handle_unknown&#39;: &#39;error&#39;, &#39;sparse&#39;: True} . import re date_str = data.game_length m = 2717 #longest games are 45m 17s for i in range(len(date_str)): if type(date_str[i]) == str: p = re.compile(&#39; d*&#39;) min = float(p.findall(date_str[i][:2])[0]) temp = p.findall(date_str[i][-3:]) for j in temp: if j != &#39;&#39;: sec = float(j) break date_str[i] = (60*min+sec)/m else: date_str[i] = date_str[i]/m # print(date_str[i]) # print(len(date_str)) . #except_champs = data.drop(columns=[&#39;result&#39;,&#39;top1&#39;,&#39;jg1&#39;,&#39;mid1&#39;,&#39;adc1&#39;,&#39;sup1&#39;,&#39;top2&#39;,&#39;jg2&#39;,&#39;mid2&#39;,&#39;adc2&#39;,&#39;sup2&#39;]) sparse_to_df = pd.DataFrame.sparse.from_spmatrix(only_champs_onehot) print(sparse_to_df.shape) print(date_str.shape) X = date_str.to_frame().join(sparse_to_df).dropna() X = np.asarray(X).astype(&#39;float32&#39;) . (4028, 754) (4028,) . y = data[&#39;result&#39;] for i in range(len(y)): if y[i] == &quot;Victory&quot;: y[i] = 1 else: y[i] = 0 . y = np.asarray(y).astype(&#39;float32&#39;) . Datas are one hot encoded and cleaned up. Let&#39;s train test split . from sklearn.model_selection import train_test_split import math X_train_full, X_test, y_train_full, y_test = train_test_split(X,y,test_size=0.2, random_state=42) #len(X_train) = 3222 l = math.floor(3222*0.8) X_valid, X_train = X_train_full[:l], X_train_full[l:] y_valid, y_train = y_train_full[:l], y_train_full[l:] print(y_valid.shape) print(X_valid.shape) . (2577,) (2577, 755) . Let&#39;s try Neural Network with dropouts . model = keras.models.Sequential([ keras.layers.Flatten(input_shape=(755,)), keras.layers.Dense(30, activation=&quot;relu&quot;, name=&quot;layer_1&quot;), keras.layers.Dropout(rate=0.2), keras.layers.Dense(16, activation=&quot;relu&quot;, name=&quot;layer_2&quot;), keras.layers.Dropout(rate=0.2), keras.layers.Dense(16, activation=&quot;relu&quot;, name=&quot;layer_3&quot;), keras.layers.Dropout(rate=0.2), keras.layers.Dense(1, activation=&quot;sigmoid&quot;, name=&quot;layer_4&quot;) ]) . model.compile(loss=&quot;binary_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics=[&quot;accuracy&quot;]) model.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten (Flatten) (None, 755) 0 _________________________________________________________________ layer_1 (Dense) (None, 30) 22680 _________________________________________________________________ dropout (Dropout) (None, 30) 0 _________________________________________________________________ layer_2 (Dense) (None, 16) 496 _________________________________________________________________ dropout_1 (Dropout) (None, 16) 0 _________________________________________________________________ layer_3 (Dense) (None, 16) 272 _________________________________________________________________ dropout_2 (Dropout) (None, 16) 0 _________________________________________________________________ layer_4 (Dense) (None, 1) 17 ================================================================= Total params: 23,465 Trainable params: 23,465 Non-trainable params: 0 _________________________________________________________________ . model.fit(X_train, y_train, epochs=50, batch_size=1) . test_loss, test_acc = model.evaluate(X_test, y_test) print(&#39;accuracy&#39;, test_acc) . 26/26 [==============================] - 0s 806us/step - loss: 3.8032 - accuracy: 0.6613 accuracy 0.6612903475761414 . We got about 0.661 accuracy with just raw neural network with dropouts. . Let&#39;s try random forests . from sklearn.ensemble import RandomForestClassifier rnd_clf = RandomForestClassifier(n_estimators=2000, max_leaf_nodes=32, n_jobs=-1) rnd_clf.fit(X_train, y_train) . RandomForestClassifier(max_leaf_nodes=32, n_estimators=2000, n_jobs=-1) . y_val_pred = rnd_clf.predict(X_valid) . val_acc = np.sum(y_val_pred == y_valid)/len(y_valid) print(&quot;validation accuracy: &quot;+str(val_acc)) . validation accuracy: 0.7710516103996896 . y_test_pred = rnd_clf.predict(X_test) test_acc = np.sum(y_test_pred == y_test)/len(y_test) print(&quot;test accuracy: &quot;+str(test_acc)) . test accuracy: 0.7704714640198511 . Immediate improvement by almost 10% with random forest classifier! . Model Explanability . Let&#39;s look at what we were mostly interested. What are some best team compositions! . # from eli5.sklearn import PermutationImportance # perm = PermutationImportance(rnd_clf, random_state=42).fit(X_valid, y_valid) # eli5.show_weights(perm, feature_names=X_valid.columns.tolist()) # Will take billions years to compute . Let&#39;s try SHAP summary . import shap explainer = shap.TreeExplainer(rnd_clf) shap_values = explainer.shap_values(X_valid) shap.summary_plot(shap_values[1], X_valid) . We see that feature 0 (game length) tells us that the game favors blue team winning more when game is shorter which is unexpected. Note that it it not significant at all since SHAP value is -0.02 ~ 0.4 at most. | Generally, since all the values are 0 are 1, we can see clear 1-red and 0-blue (When it&#39;s 0 it has no impact on the prediction) | We can see feature 156(blue Mid Akali) helped RED team win more | Whereas Feature 462(red Top Tryndamere) helps the BLUE team win significantly more haha | From this chart, we can clearly see that each champion has very consistent and predictable contribution to their team&#39;s chance of winning | . Note that . 119 Kindred blue jg | 638 Caitlyn red adc | 60 Renekton blue top | 162 Cassiopeia blue mid | 535 Akali red mid | 376 Thresh blue support | 471 Volibear red top | 31 Jax blue top | 654 Kalista red adc | 290 Miss Fortune blue adc | 259 Ashe blue adc | 360 Rakan blue support | 210 Orianna blue mid | 462 Tryndamere red top | 445 Riven red top | 425 Lucian red top | 715 Janna red support | 156 Akali blue mid | 72 Sylas blue top | . Therefore our best teamp comp impacting positively on winning is ... . (Top)Renekton/Jax/Sylas (Jg)Kindred (Mid) Cassiopeia/Orianna (Adc)MF/Ashe (Sup)Thresh/Rakan | . Meanwhile worst team comp impacting negatively on winning is ... . (Top)Volibear/Trynd/Riven/Lucian (Mid)Akali (Adc)Caitlyn/Kalista (Sup)Janna | . We can also note that Jg role seem to not matter much.. : ) . def find_champ(i): temp_list = [99,54,101,73,57,96,52,102,68,52] for num in range(len(temp_list)): if (i-temp_list[num] &lt;= 0): return enc.categories_[num][i-1] else: i = i-temp_list[num] . # list_champ = [119, 638,60,162,535,376,471,31,654,290,259,360,210,462,445,425,715,156,72] # for champ in list_champ: # lane = &#39;&#39; # if champ &lt;= 99 or 385&lt;=champ&lt;=480: # lane = &quot;top&quot; # elif 100 &lt;= champ &lt;=153 or 481&lt;=champ&lt;=532: # lane = &quot;jg&quot; # elif 154 &lt;= champ &lt;=255 or 533&lt;=champ&lt;=634: # lane= &quot;mid&quot; # elif 256 &lt;= champ &lt;=327 or 635&lt;=champ&lt;=702: # lane= &quot;adc&quot; # else: # lane = &quot;support&quot; # team = &quot;blue&quot; if champ &lt;= 384 else &quot;red&quot; # print(champ, find_champ(champ), team, lane) #print(len(enc.categories_[0])) 99 #print(len(enc.categories_[1])) 54 //153 #print(len(enc.categories_[2])) 101 //254 #print(len(enc.categories_[3])) 73 //327 #print(len(enc.categories_[4])) 57 // UP TO 384 is blue team #print(len(enc.categories_[5])) 96 //480 #print(len(enc.categories_[6])) 52 //532 #print(len(enc.categories_[7])) 102 //634 #print(len(enc.categories_[8])) 68 //702 #print(len(enc.categories_[9])) 52 //754 . What if we didn&#39;t have game length, just champion compositions only? . X_1 = sparse_to_df . X_train_full, X_test, y_train_full, y_test = train_test_split(X_1,y,test_size=0.2, random_state=42) #len(X_train) = 3222 l = math.floor(3222*0.8) X_valid, X_train = X_train_full[:l], X_train_full[l:] y_valid, y_train = y_train_full[:l], y_train_full[l:] print(y_valid.shape) print(X_valid.shape) . (2577,) (2577, 754) . rnd_clf = RandomForestClassifier(n_estimators=2000, max_leaf_nodes=32, n_jobs=-1) rnd_clf.fit(X_train, y_train) . RandomForestClassifier(max_leaf_nodes=32, n_estimators=2000, n_jobs=-1) . y_val_pred = rnd_clf.predict(X_valid) val_acc = np.sum(y_val_pred == y_valid)/len(y_valid) print(&quot;validation accuracy: &quot;+str(val_acc)) . validation accuracy: 0.7691113698098564 . y_test_pred = rnd_clf.predict(X_test) test_acc = np.sum(y_test_pred == y_test)/len(y_test) print(&quot;test accuracy: &quot;+str(test_acc)) . test accuracy: 0.7692307692307693 . Surprisingly, accuracy only drops less than 0.01. We can conclude that planning out a team comp based on champion&#39;s strength on early vs late game does not help win more. This can be explained by an example. Let&#39;s say I picked kayle which is the best late game champion. We may win games with longer duration more but will lose more short games due to her weakness early. So the overall win rate balances out. . Conclusion . best: (Top)Renekton/Jax/Sylas (Jg)Kindred (Mid) Cassiopeia/Orianna (Adc)MF/Ashe (Sup)Thresh/Rakan | worst: (Top)Volibear/Trynd/Riven/Lucian (Mid)Akali (Adc)Caitlyn/Kalista (Sup)Janna | . We know that in the world of solo queue, picking the above champions will not gurantee a win. Sometimes people are autofilled, meaning they aren&#39;t playing on their best role. People may disconnect, resulting in games favoring the opposite team. There are too many unknown factors like this, making it impossible to predict 100% of the game outcomes correctly. . As a former high elo NA player myself, I can say that generally, the &#39;best team&#39; above have champions that doesn&#39;t get countered too often and is a good pick into anything. (This may not be the case for top because I&#39;ve never really cared about top lanes as a support player :). But for &#39;worst team&#39; champions, they are often easily countered. (Especially bottom lane) . The biggest surprise was blue team wins more early and red team wins more late (Very slightly but certainly) for some reason. Also jg mattering the least was a surprise as well. .",
            "url": "https://leejaeka.github.io/jaekangai/fastpages/jupyter/2020/10/28/lolpredict.html",
            "relUrl": "/fastpages/jupyter/2020/10/28/lolpredict.html",
            "date": " ‚Ä¢ Oct 28, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Renekton Croc plush detector üêä",
            "content": "reptile_types = &#39;crocodile&#39;,&#39;alligator plush&#39;, &#39;renekton&#39; path = Path(&#39;reptiles&#39;) . if not path.exists(): path.mkdir() for o in reptile_types: dest = (path/o) dest.mkdir(exist_ok=True) results = search_images_bing(key, f&#39;{o}&#39;) download_images(dest, urls=results.attrgot(&#39;content_url&#39;)) . fns = get_image_files(path) fns . (#416) [Path(&#39;reptiles/alligator plush/00000000.jpg&#39;),Path(&#39;reptiles/alligator plush/00000001.jpg&#39;),Path(&#39;reptiles/alligator plush/00000002.jpg&#39;),Path(&#39;reptiles/alligator plush/00000003.jpg&#39;),Path(&#39;reptiles/alligator plush/00000004.jpeg&#39;),Path(&#39;reptiles/alligator plush/00000005.jpg&#39;),Path(&#39;reptiles/alligator plush/00000006.jpg&#39;),Path(&#39;reptiles/alligator plush/00000007.jpg&#39;),Path(&#39;reptiles/alligator plush/00000008.jpg&#39;),Path(&#39;reptiles/alligator plush/00000009.jpg&#39;)...] . failed = verify_images(fns) failed.map(Path.unlink) . (#0) [] . class DataLoaders(GetAttr): num_workers=0 def __init__(self, *loaders): self.loaders = loaders def __getitem__(self, i): return self.loaders[i] train,valid = add_props(lambda i, self: self[i]) . reptiles = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(128)) . dls = reptiles.dataloaders(path) dls.valid.show_batch(max_n=4, nrows=1) . reptiles = reptiles.new(item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) dls = reptiles.dataloaders(path, num_workers=0) # &lt;- num_workers=0 to prevent window error . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . epoch train_loss valid_loss error_rate time . 0 | 1.455003 | 0.249432 | 0.084337 | 00:18 | . epoch train_loss valid_loss error_rate time . 0 | 0.104521 | 0.049535 | 0.024096 | 00:18 | . 1 | 0.068319 | 0.012980 | 0.012048 | 00:18 | . 2 | 0.052283 | 0.011862 | 0.000000 | 00:19 | . 3 | 0.041685 | 0.010840 | 0.000000 | 00:19 | . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . Result is very good . # cleaner # for idx in cleaner.delete(): cleaner.fns[idx].unlink() # delete . Let&#39;s test . my_renek = PILImage.create(&quot;renek_plush.png&quot;) display(my_renek.to_thumb(256,256)) . pred, pred_idx, probs =learn.predict(my_renek) f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; . &#39;Prediction: alligator plush; Probability: 0.9391&#39; . Very good. It is very accurate since my drawing of a plush is very realistic. . renek = PILImage.create(&quot;renek_test.png&quot;) display(renek.to_thumb(256,256)) pred, pred_idx, probs =learn.predict(renek) f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; . &#39;Prediction: renekton; Probability: 0.9834&#39; . Easily recognizes my drawing of Renekton as well. I guess I&#39;m an artist . renek_withoutbg = PILImage.create(&quot;renek_test1.png&quot;) display(renek_withoutbg.to_thumb(256,256)) pred, pred_idx, probs =learn.predict(renek_withoutbg) f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; . &#39;Prediction: renekton; Probability: 0.9674&#39; . Expected the model to predict plush becasue I removed the background but it&#39;s too smart. (In dataset a lot of plush had empty white background contrast to lots of Renekton images having dark backgrounds) . beard = PILImage.create(&quot;beard.jpg&quot;) display(beard.to_thumb(200,200)) pred, pred_idx, probs =learn.predict(beard) f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; . &#39;Prediction: alligator plush; Probability: 0.8644&#39; . Indeed I am an alligator plush with my fake beard! . learn.export() . RUN CODE BELOW TO MAKE YOUR OWN TEST (Download export.pkl file on my github) . from fastai.vision.widgets import * btn_upload = widgets.FileUpload() out_pl = widgets.Output() lbl_pred = widgets.Label() . path = Path(&#39;&#39;) learn_inf = load_learner(path/&#39;export.pkl&#39;, cpu=True) . def on_data_change(change): lbl_pred.value = &#39;&#39; img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) pred,pred_idx,probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; . btn_upload.observe(on_data_change, names=[&#39;data&#39;]) display(VBox([widgets.Label(&#39;Feed me a reptile photo!&#39;), btn_upload, out_pl, lbl_pred])) .",
            "url": "https://leejaeka.github.io/jaekangai/2020/10/27/croc.html",
            "relUrl": "/2020/10/27/croc.html",
            "date": " ‚Ä¢ Oct 27, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "City Detector üèôÔ∏è",
            "content": "Let&#39;s make app that recognizes Coquitlam! . Let&#39;s get the images of each city . city_types = &#39;seoul city&#39;,&#39;coquitlam&#39;,&#39;paris city&#39;, &#39;new york city&#39; path = Path(&#39;cities&#39;) . if not path.exists(): path.mkdir() for o in city_types: dest = (path/o) dest.mkdir(exist_ok=True) results = search_images_bing(key, f&#39;{o}&#39;) download_images(dest, urls=results.attrgot(&#39;content_url&#39;)) . fns = get_image_files(path) fns . failed = verify_images(fns) failed . failed.map(Path.unlink) . (#16) [None,None,None,None,None,None,None,None,None,None...] . Let&#39;s load the data . class DataLoaders(GetAttr): def __init__(self, *loaders): self.loaders = loaders def __getitem__(self, i): return self.loaders[i] train,valid = add_props(lambda i, self: self[i]) . cities = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(128)) . dls = cities.dataloaders(path) dls.valid.show_batch(max_n=8, nrows = 2) . Let&#39;s scale and augment the datas . cities = cities.new(item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) dls = cities.dataloaders(path, num_workers=0) # &lt;- num_workers=0 to prevent window error . Let&#39;s build and run a CNN model . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . epoch train_loss valid_loss error_rate time . 0 | 2.187395 | 1.315027 | 0.482759 | 00:42 | . epoch train_loss valid_loss error_rate time . 0 | 1.307873 | 0.871226 | 0.336207 | 00:41 | . 1 | 1.064780 | 0.831430 | 0.241379 | 00:41 | . 2 | 0.876646 | 0.767134 | 0.215517 | 00:41 | . 3 | 0.784991 | 0.738216 | 0.224138 | 00:49 | . Let&#39;s look at the confusion matrix . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . We got an accuracy of 26/90 = 71% (rounded) . interp.plot_top_losses(2,nrows=2) . Let&#39;s try to clean up the dataset . cleaner = ImageClassifierCleaner(learn) cleaner . for idx in cleaner.delete(): cleaner.fns[idx].unlink() # delete . dls = cities.dataloaders(path, num_workers=0) learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(5) . epoch train_loss valid_loss error_rate time . 0 | 1.933403 | 1.322945 | 0.460870 | 00:38 | . epoch train_loss valid_loss error_rate time . 0 | 1.258363 | 0.800413 | 0.347826 | 00:37 | . 1 | 1.014135 | 0.660854 | 0.243478 | 00:38 | . 2 | 0.851025 | 0.609896 | 0.243478 | 00:38 | . 3 | 0.725140 | 0.591347 | 0.217391 | 00:37 | . 4 | 0.623130 | 0.582418 | 0.226087 | 00:37 | . Valid_loss doesn&#39;t decrease so we stop . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . interp.plot_top_losses(6,nrows=2) . It looks like it has a hard time highlighting seoul city&#39;s characteristics as most error comes from seoul images. Suspected factors include seoul having new york like buildings, mountains like coquitlam and brick structures like paris city. . Ignore Below (Deployment IPR) . learn.export() . path = Path() path.ls(file_exts=&#39;.pkl&#39;) . learn_inf = load_learner(path/&#39;export.pkl&#39;) . learn_inf.dls.vocab . [&#39;coquitlam&#39;, &#39;new york city&#39;, &#39;paris city&#39;, &#39;seoul city&#39;] . btn_upload = widgets.FileUpload() btn_upload . img = PILImage.create(btn_upload.data[-1]) . out_pl = widgets.Output() out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) out_pl . pred,pred_idx,probs = learn_inf.predict(img) #hide_output lbl_pred = widgets.Label() lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; lbl_pred . Classify Button &amp; Event Handler . btn_run = widgets.Button(description=&#39;Classify&#39;) btn_run . def on_click_classify(change): img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) pred,pred_idx,probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; btn_run.on_click(on_click_classify) .",
            "url": "https://leejaeka.github.io/jaekangai/fastpages/jupyter/2020/10/27/city.html",
            "relUrl": "/fastpages/jupyter/2020/10/27/city.html",
            "date": " ‚Ä¢ Oct 27, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am currently a student at the University of Toronto studying Mathematic, Statistic and Computer Science. I am currently studying data science and machine learning : ) . Tip: Check out my jellyfish tank on my other website! www.jaekanglee.com .",
          "url": "https://leejaeka.github.io/jaekangai/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://leejaeka.github.io/jaekangai/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}